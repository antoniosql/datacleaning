{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e55d3863",
   "metadata": {},
   "source": [
    "# FraSoHome ‚Äî Notebook 7: Baseline Modeling (Churn & Propensi√≥n)\n",
    "\n",
    "> **Objetivo formativo:** construir **modelos baseline** r√°pidos y explicables (sin ‚Äúm√°gia‚Äù), medir rendimiento con m√©tricas est√°ndar y discutir **fugas de informaci√≥n (data leakage)**, particionado temporal/grupal, umbrales, etc.\n",
    "\n",
    "Este notebook asume que ya ejecutaste:\n",
    "- **Notebook 5** (features) ‚Üí genera `output_features/*.csv`\n",
    "- **Notebook 6** (preprocesado) ‚Üí genera `output_ml/*.csv`\n",
    "\n",
    "Aun as√≠, el notebook intenta ser **robusto** y te avisa si faltan archivos.\n",
    "\n",
    "---\n",
    "\n",
    "## Contenidos\n",
    "\n",
    "1. Carga de datasets ML-ready (`output_ml/`)\n",
    "2. Modelado baseline de **churn** (clientes)\n",
    "3. Modelado baseline de **propensi√≥n de compra** (snapshots)\n",
    "4. Comparaci√≥n de modelos, umbral y explicabilidad simple\n",
    "5. Export de m√©tricas y (opcional) guardado del mejor modelo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd72d997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 0) Setup\n",
    "# ============================================\n",
    "from __future__ import annotations\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GroupShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score,\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "try:\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    _HAS_PERM = True\n",
    "except Exception:\n",
    "    _HAS_PERM = False\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "BASE_DIR = Path(\".\")  # ajusta si ejecutas desde otra carpeta\n",
    "OUTPUT_ML_DIR = BASE_DIR / \"output_ml\"\n",
    "OUTPUT_MODELS_DIR = BASE_DIR / \"output_models\"\n",
    "OUTPUT_MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CLIENTS_ML_PATH = OUTPUT_ML_DIR / \"FraSoHome_clientes_ML_ready.csv\"\n",
    "PROP_ML_PATH = OUTPUT_ML_DIR / \"FraSoHome_propension_ML_ready.csv\"\n",
    "\n",
    "print(\"üìÅ output_ml:\", OUTPUT_ML_DIR.resolve())\n",
    "print(\" - clientes:\", CLIENTS_ML_PATH.exists(), CLIENTS_ML_PATH)\n",
    "print(\" - propensi√≥n:\", PROP_ML_PATH.exists(), PROP_ML_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546e9bee",
   "metadata": {},
   "source": [
    "## 1) Funciones reutilizables (estructura)\n",
    "\n",
    "Todas las funciones est√°n pensadas para ser **reutilizables** y trabajar con **dataframes como par√°metros**.\n",
    "\n",
    "- Carga robusta / estandarizaci√≥n ligera\n",
    "- Detecci√≥n del target\n",
    "- Split (random / por grupo / temporal si existe `snapshot_date`)\n",
    "- Entrenamiento y evaluaci√≥n (AUC-ROC, PR-AUC, F1, etc.)\n",
    "- Selecci√≥n de umbral\n",
    "- Importancia de variables (coeficientes, Gini, permutation importance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858162ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 1) Helpers reutilizables\n",
    "# ============================================\n",
    "\n",
    "def standardize_column_names(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip().lower() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def load_csv_robust(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Carga CSV como texto para no romperse con formatos raros; deja el tipado para despu√©s.\"\"\"\n",
    "    df = pd.read_csv(path, dtype=str, encoding=\"utf-8\")\n",
    "    df = standardize_column_names(df)\n",
    "    return df\n",
    "\n",
    "def infer_target_column(df: pd.DataFrame, preferred: str | None, candidates: list[str]) -> str | None:\n",
    "    cols = set(df.columns)\n",
    "    if preferred and preferred in cols:\n",
    "        return preferred\n",
    "    for c in candidates:\n",
    "        if c in cols:\n",
    "            return c\n",
    "    # heur√≠stica: primer label_*\n",
    "    label_cols = [c for c in df.columns if c.startswith(\"label_\")]\n",
    "    return label_cols[0] if label_cols else None\n",
    "\n",
    "def to_numeric_df(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Intenta convertir todas las columnas a num√©rico; si no, coerciona a NaN.\"\"\"\n",
    "    Xn = X.copy()\n",
    "    for c in Xn.columns:\n",
    "        Xn[c] = pd.to_numeric(Xn[c], errors=\"coerce\")\n",
    "    return Xn\n",
    "\n",
    "def split_X_y(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str,\n",
    "    id_cols: list[str] | None = None,\n",
    "    drop_cols: list[str] | None = None\n",
    ") -> tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"Separa X e y; elimina IDs y columnas a descartar.\"\"\"\n",
    "    df = df.copy()\n",
    "    y = df[target_col].copy()\n",
    "    X = df.drop(columns=[target_col])\n",
    "\n",
    "    cols_to_drop = set()\n",
    "    if id_cols:\n",
    "        cols_to_drop |= set([c for c in id_cols if c in X.columns])\n",
    "    if drop_cols:\n",
    "        cols_to_drop |= set([c for c in drop_cols if c in X.columns])\n",
    "\n",
    "    if cols_to_drop:\n",
    "        X = X.drop(columns=sorted(cols_to_drop))\n",
    "\n",
    "    # Convierto a num√©rico (ML-ready deber√≠a venir ya num√©rico)\n",
    "    X = to_numeric_df(X)\n",
    "\n",
    "    # Target a 0/1\n",
    "    y_num = pd.to_numeric(y, errors=\"coerce\")\n",
    "    if y_num.isna().any():\n",
    "        # intenta mapear strings t√≠picos\n",
    "        y_map = (y.astype(str).str.strip().str.lower()\n",
    "                 .replace({\"true\": 1, \"false\": 0, \"yes\": 1, \"no\": 0, \"si\": 1, \"s√≠\": 1, \"n\": 0, \"s\": 1}))\n",
    "        y_num = pd.to_numeric(y_map, errors=\"coerce\")\n",
    "    return X, y_num\n",
    "\n",
    "def print_basic_dataset_report(df: pd.DataFrame, name: str, target_col: str | None = None) -> None:\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"shape:\", df.shape)\n",
    "    print(\"cols:\", len(df.columns))\n",
    "    print(\"nulos totales:\", int(df.isna().sum().sum()))\n",
    "    if target_col and target_col in df.columns:\n",
    "        y = pd.to_numeric(df[target_col], errors=\"coerce\")\n",
    "        print(\"target:\", target_col)\n",
    "        print(\"clases (incl NaN):\")\n",
    "        print(y.value_counts(dropna=False).head(10))\n",
    "\n",
    "def evaluate_binary_classifier(\n",
    "    model,\n",
    "    X_train: pd.DataFrame, y_train: pd.Series,\n",
    "    X_test: pd.DataFrame, y_test: pd.Series,\n",
    "    threshold: float = 0.5,\n",
    "    name: str = \"model\"\n",
    ") -> dict:\n",
    "    \"\"\"Entrena, predice y calcula m√©tricas est√°ndar.\"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # proba positiva (col=1)\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        p_test = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        # fallback a decision_function\n",
    "        s = model.decision_function(X_test)\n",
    "        p_test = (s - s.min()) / (s.max() - s.min() + 1e-9)\n",
    "\n",
    "    y_pred = (p_test >= threshold).astype(int)\n",
    "\n",
    "    metrics = {\n",
    "        \"model\": name,\n",
    "        \"roc_auc\": roc_auc_score(y_test, p_test) if y_test.nunique() > 1 else np.nan,\n",
    "        \"pr_auc\": average_precision_score(y_test, p_test) if y_test.nunique() > 1 else np.nan,\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"f1\": f1_score(y_test, y_pred, zero_division=0),\n",
    "        \"precision\": precision_score(y_test, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_test, y_pred, zero_division=0),\n",
    "        \"threshold\": threshold,\n",
    "    }\n",
    "\n",
    "    print(f\"\\n--- {name} @ threshold={threshold:.2f} ---\")\n",
    "    print(pd.Series(metrics).drop([\"model\"]).round(4))\n",
    "    print(\"\\nConfusion matrix (rows=true, cols=pred):\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "    return {\"metrics\": metrics, \"proba_test\": p_test, \"y_pred\": y_pred, \"fitted_model\": model}\n",
    "\n",
    "def threshold_sweep(y_true: pd.Series, y_prob: np.ndarray, metric: str = \"f1\") -> tuple[float, pd.DataFrame]:\n",
    "    \"\"\"Busca el mejor umbral para una m√©trica simple (f1 o recall o precision).\"\"\"\n",
    "    y_true = pd.Series(y_true).astype(int).values\n",
    "    rows = []\n",
    "    for t in np.linspace(0.05, 0.95, 19):\n",
    "        y_pred = (y_prob >= t).astype(int)\n",
    "        row = {\n",
    "            \"threshold\": float(t),\n",
    "            \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "            \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "            \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "            \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        }\n",
    "        rows.append(row)\n",
    "    df = pd.DataFrame(rows)\n",
    "    if metric not in df.columns:\n",
    "        metric = \"f1\"\n",
    "    best_t = float(df.sort_values(metric, ascending=False).iloc[0][\"threshold\"])\n",
    "    return best_t, df.sort_values(metric, ascending=False)\n",
    "\n",
    "def plot_score_distributions(y_true: pd.Series, y_prob: np.ndarray, title: str) -> None:\n",
    "    y_true = pd.Series(y_true).astype(int).values\n",
    "    plt.figure()\n",
    "    plt.hist(y_prob[y_true == 0], bins=20, alpha=0.6, label=\"Clase 0\")\n",
    "    plt.hist(y_prob[y_true == 1], bins=20, alpha=0.6, label=\"Clase 1\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Probabilidad estimada\")\n",
    "    plt.ylabel(\"Frecuencia\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def safe_feature_importance(model, feature_names: list[str], top_n: int = 20) -> pd.DataFrame:\n",
    "    \"\"\"Devuelve importancias si el modelo las tiene (coef o feature_importances_).\"\"\"\n",
    "    rows = []\n",
    "    m = model\n",
    "    # si es pipeline, toma el √∫ltimo paso\n",
    "    if hasattr(model, \"named_steps\"):\n",
    "        m = list(model.named_steps.values())[-1]\n",
    "\n",
    "    if hasattr(m, \"coef_\"):\n",
    "        coefs = m.coef_.ravel()\n",
    "        rows = [{\"feature\": f, \"importance\": float(w)} for f, w in zip(feature_names, coefs)]\n",
    "    elif hasattr(m, \"feature_importances_\"):\n",
    "        imps = m.feature_importances_\n",
    "        rows = [{\"feature\": f, \"importance\": float(w)} for f, w in zip(feature_names, imps)]\n",
    "    else:\n",
    "        return pd.DataFrame(columns=[\"feature\", \"importance\"])\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df[\"abs_importance\"] = df[\"importance\"].abs()\n",
    "    return df.sort_values(\"abs_importance\", ascending=False).head(top_n)[[\"feature\", \"importance\", \"abs_importance\"]]\n",
    "\n",
    "def permutation_importance_report(model, X_test: pd.DataFrame, y_test: pd.Series, top_n: int = 20) -> pd.DataFrame:\n",
    "    if not _HAS_PERM:\n",
    "        print(\"‚ÑπÔ∏è permutation_importance no disponible en este entorno.\")\n",
    "        return pd.DataFrame(columns=[\"feature\", \"importance_mean\", \"importance_std\"])\n",
    "\n",
    "    r = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "    df = pd.DataFrame({\n",
    "        \"feature\": X_test.columns,\n",
    "        \"importance_mean\": r.importances_mean,\n",
    "        \"importance_std\": r.importances_std\n",
    "    })\n",
    "    return df.sort_values(\"importance_mean\", ascending=False).head(top_n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19874a71",
   "metadata": {},
   "source": [
    "## 2) Caso de uso 1: **Churn** (clientes)\n",
    "\n",
    "Trabajaremos con `output_ml/FraSoHome_clientes_ML_ready.csv`.\n",
    "\n",
    "- **Target esperado:** `label_churn_180d` (0/1)\n",
    "- Se eliminan columnas ID si existen (`customer_id`, etc.)\n",
    "- Split estratificado train/test\n",
    "- Comparaci√≥n de baselines:\n",
    "  - Dummy (mayor√≠a)\n",
    "  - Logistic Regression (baseline interpretable)\n",
    "  - RandomForest (baseline no lineal)\n",
    "  - GradientBoosting (baseline boosting cl√°sico)\n",
    "\n",
    "> Nota formativa: al ser un dataset sint√©tico y con ‚Äúruido‚Äù intencional, las m√©tricas pueden variar bastante. Lo importante es practicar el **proceso**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4e70d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2) Carga dataset churn\n",
    "# ============================================\n",
    "if not CLIENTS_ML_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"No existe {CLIENTS_ML_PATH}. Ejecuta antes el Notebook 6 (output_ml/) o ajusta BASE_DIR.\"\n",
    "    )\n",
    "\n",
    "df_churn = load_csv_robust(CLIENTS_ML_PATH)\n",
    "print_basic_dataset_report(df_churn, \"clientes_ml_ready\", target_col=\"label_churn_180d\")\n",
    "\n",
    "# Detecta target\n",
    "target_churn = infer_target_column(\n",
    "    df_churn,\n",
    "    preferred=\"label_churn_180d\",\n",
    "    candidates=[\"target\", \"y\", \"churn\", \"will_churn\"]\n",
    ")\n",
    "if not target_churn:\n",
    "    raise ValueError(\"No se encontr√≥ columna target para churn. Revisa el dataset.\")\n",
    "\n",
    "# IDs t√≠picos: si existen, los quitamos de X\n",
    "id_cols_churn = [c for c in df_churn.columns if c in {\"customer_id\", \"cliente_id\", \"id_cliente\"} or c.endswith(\"_id\")]\n",
    "\n",
    "# Construye X,y\n",
    "X_churn, y_churn = split_X_y(df_churn, target_col=target_churn, id_cols=id_cols_churn)\n",
    "\n",
    "# Elimina filas donde y sea NaN (si el label ven√≠a corrupto)\n",
    "mask = y_churn.notna()\n",
    "X_churn = X_churn.loc[mask].copy()\n",
    "y_churn = y_churn.loc[mask].astype(int).copy()\n",
    "\n",
    "print(\"\\nX_churn shape:\", X_churn.shape)\n",
    "print(\"y_churn balance:\")\n",
    "print(y_churn.value_counts(normalize=True).round(3))\n",
    "\n",
    "# Split estratificado\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_churn, y_churn,\n",
    "    test_size=0.25,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y_churn\n",
    ")\n",
    "\n",
    "print(\"\\nTrain:\", X_train.shape, \"Test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939f25f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2.1) Modelos baseline churn\n",
    "# ============================================\n",
    "models_churn = {\n",
    "    \"dummy_most_frequent\": Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"model\", DummyClassifier(strategy=\"most_frequent\", random_state=RANDOM_STATE))\n",
    "    ]),\n",
    "    \"logreg_balanced\": Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        # Si Notebook 6 ya escal√≥, esto no es imprescindible. Se deja por robustez.\n",
    "        (\"scaler\", StandardScaler(with_mean=False)),\n",
    "        (\"model\", LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=RANDOM_STATE))\n",
    "    ]),\n",
    "    \"random_forest\": Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"model\", RandomForestClassifier(\n",
    "            n_estimators=400,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1,\n",
    "            class_weight=\"balanced_subsample\"\n",
    "        ))\n",
    "    ]),\n",
    "    \"grad_boost\": Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"model\", GradientBoostingClassifier(random_state=RANDOM_STATE))\n",
    "    ]),\n",
    "}\n",
    "\n",
    "results_churn = []\n",
    "fitted_churn = {}\n",
    "\n",
    "for name, model in models_churn.items():\n",
    "    out = evaluate_binary_classifier(\n",
    "        model=model,\n",
    "        X_train=X_train, y_train=y_train,\n",
    "        X_test=X_test, y_test=y_test,\n",
    "        threshold=0.50,\n",
    "        name=name\n",
    "    )\n",
    "    results_churn.append(out[\"metrics\"])\n",
    "    fitted_churn[name] = out\n",
    "\n",
    "df_results_churn = pd.DataFrame(results_churn).sort_values([\"roc_auc\", \"pr_auc\"], ascending=False)\n",
    "df_results_churn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c268029a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2.2) Ajuste simple de umbral (sobre el mejor modelo por ROC-AUC)\n",
    "# ============================================\n",
    "best_model_name = df_results_churn.iloc[0][\"model\"]\n",
    "best = fitted_churn[best_model_name]\n",
    "p_test = best[\"proba_test\"]\n",
    "\n",
    "best_t, sweep = threshold_sweep(y_test, p_test, metric=\"f1\")\n",
    "print(\"Mejor umbral (F1):\", best_t)\n",
    "sweep.head(10)\n",
    "\n",
    "# Re-eval con el umbral √≥ptimo\n",
    "_ = evaluate_binary_classifier(\n",
    "    model=best[\"fitted_model\"],\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_test=X_test, y_test=y_test,\n",
    "    threshold=best_t,\n",
    "    name=f\"{best_model_name}_tuned\"\n",
    ")\n",
    "\n",
    "plot_score_distributions(y_test, p_test, title=f\"Distribuci√≥n de scores - {best_model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da52dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2.3) Explicabilidad simple (importancia de variables)\n",
    "# ============================================\n",
    "# OJO: en pipelines con scaler, las importancias siguen siendo interpretables (coef), pero en escala estandarizada.\n",
    "best_fitted = best[\"fitted_model\"]\n",
    "top_imp = safe_feature_importance(best_fitted, feature_names=list(X_train.columns), top_n=25)\n",
    "print(\"Top importancias (si aplica):\")\n",
    "display(top_imp)\n",
    "\n",
    "if _HAS_PERM:\n",
    "    perm = permutation_importance_report(best_fitted, X_test, y_test, top_n=25)\n",
    "    print(\"\\nPermutation importance (top 25):\")\n",
    "    display(perm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f510be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2.4) Export de m√©tricas churn + (opcional) guardado del modelo\n",
    "# ============================================\n",
    "churn_metrics_path = OUTPUT_MODELS_DIR / \"churn_metrics_baseline.csv\"\n",
    "df_results_churn.to_csv(churn_metrics_path, index=False, encoding=\"utf-8\")\n",
    "print(\"‚úÖ Exportado:\", churn_metrics_path)\n",
    "\n",
    "# Guardar modelo (joblib)\n",
    "try:\n",
    "    import joblib\n",
    "    churn_model_path = OUTPUT_MODELS_DIR / f\"churn_model_{best_model_name}.joblib\"\n",
    "    joblib.dump(best_fitted, churn_model_path)\n",
    "    print(\"‚úÖ Modelo guardado:\", churn_model_path)\n",
    "except Exception as e:\n",
    "    print(\"‚ÑπÔ∏è No se pudo guardar el modelo (joblib). Motivo:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2b4f7c",
   "metadata": {},
   "source": [
    "## 3) Caso de uso 2: **Propensi√≥n de compra** (snapshots)\n",
    "\n",
    "Trabajaremos con `output_ml/FraSoHome_propension_ML_ready.csv` si existe.\n",
    "\n",
    "- El notebook intenta detectar el **target** autom√°ticamente:\n",
    "  - primera columna `label_*`, o `target`, `y`, `will_buy`\n",
    "- Split **recomendado** (formativo):\n",
    "  - si existe `snapshot_date`: split **temporal** (√∫ltimo tramo como test)\n",
    "  - si existe `customer_id`: split por **grupo** (evitar leakage por cliente)\n",
    "  - si no: split aleatorio estratificado\n",
    "\n",
    "> Esto es crucial: en problemas de propensi√≥n, un split aleatorio puede ‚Äúfiltrar futuro‚Äù de forma impl√≠cita si hay m√∫ltiples snapshots del mismo cliente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b88bb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3) Carga dataset propensi√≥n (si existe)\n",
    "# ============================================\n",
    "if not PROP_ML_PATH.exists():\n",
    "    print(\"‚ÑπÔ∏è No existe dataset de propensi√≥n ML-ready. Ejecuta Notebook 5+6 para generarlo.\")\n",
    "    df_prop = None\n",
    "else:\n",
    "    df_prop = load_csv_robust(PROP_ML_PATH)\n",
    "    print_basic_dataset_report(df_prop, \"propension_ml_ready\", target_col=None)\n",
    "\n",
    "df_prop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24a3651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3.1) Split recomendado (temporal/grupal si se puede)\n",
    "# ============================================\n",
    "def split_for_propensity(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str,\n",
    "    date_col: str | None = \"snapshot_date\",\n",
    "    group_col: str | None = \"customer_id\",\n",
    "    test_size: float = 0.25\n",
    "):\n",
    "    df = df.copy()\n",
    "\n",
    "    # si hay fecha, intenta split temporal\n",
    "    if date_col and date_col in df.columns:\n",
    "        dt = pd.to_datetime(df[date_col], errors=\"coerce\", dayfirst=True)\n",
    "        df[\"_snapshot_dt\"] = dt\n",
    "        df = df.sort_values(\"_snapshot_dt\")\n",
    "\n",
    "        # cutoff por percentil (√∫ltimo 25% como test)\n",
    "        cutoff = df[\"_snapshot_dt\"].quantile(1 - test_size)\n",
    "        train_df = df[df[\"_snapshot_dt\"] <= cutoff].copy()\n",
    "        test_df = df[df[\"_snapshot_dt\"] > cutoff].copy()\n",
    "\n",
    "        # si por errores queda vac√≠o, fallback\n",
    "        if len(train_df) > 50 and len(test_df) > 10:\n",
    "            return train_df, test_df, f\"temporal (cutoff={cutoff.date() if pd.notna(cutoff) else cutoff})\"\n",
    "\n",
    "    # si hay grupo, split por cliente\n",
    "    if group_col and group_col in df.columns:\n",
    "        gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=RANDOM_STATE)\n",
    "        groups = df[group_col].astype(str)\n",
    "        idx_train, idx_test = next(gss.split(df, groups=groups))\n",
    "        train_df = df.iloc[idx_train].copy()\n",
    "        test_df = df.iloc[idx_test].copy()\n",
    "        return train_df, test_df, f\"group ({group_col})\"\n",
    "\n",
    "    # fallback aleatorio estratificado\n",
    "    train_df, test_df = train_test_split(\n",
    "        df, test_size=test_size, random_state=RANDOM_STATE,\n",
    "        stratify=pd.to_numeric(df[target_col], errors=\"coerce\")\n",
    "    )\n",
    "    return train_df.copy(), test_df.copy(), \"random stratified\"\n",
    "\n",
    "if df_prop is None:\n",
    "    print(\"‚ÑπÔ∏è Saltando secci√≥n de propensi√≥n.\")\n",
    "else:\n",
    "    # target\n",
    "    target_prop = infer_target_column(df_prop, preferred=None, candidates=[\"target\", \"y\", \"will_buy\"])\n",
    "    if not target_prop:\n",
    "        raise ValueError(\"No se pudo inferir target en propensi√≥n. A√±ade una columna label_* en el dataset.\")\n",
    "\n",
    "    # IDs potenciales\n",
    "    id_cols_prop = [c for c in df_prop.columns if c.endswith(\"_id\") or c in {\"customer_id\", \"product_id\", \"snapshot_date\"}]\n",
    "\n",
    "    # Pre-split (para preservar snapshot_date/customer_id antes de convertir a num√©rico)\n",
    "    train_df, test_df, split_strategy = split_for_propensity(df_prop, target_col=target_prop)\n",
    "    print(\"‚úÖ Split estrategia:\", split_strategy)\n",
    "    print(\"train/test:\", train_df.shape, test_df.shape)\n",
    "\n",
    "    # X,y para train y test\n",
    "    X_train_p, y_train_p = split_X_y(train_df, target_col=target_prop, id_cols=id_cols_prop)\n",
    "    X_test_p, y_test_p = split_X_y(test_df, target_col=target_prop, id_cols=id_cols_prop)\n",
    "\n",
    "    # limpia NaNs en y\n",
    "    mtr = y_train_p.notna()\n",
    "    mte = y_test_p.notna()\n",
    "    X_train_p, y_train_p = X_train_p.loc[mtr], y_train_p.loc[mtr].astype(int)\n",
    "    X_test_p, y_test_p = X_test_p.loc[mte], y_test_p.loc[mte].astype(int)\n",
    "\n",
    "    print(\"X_train_p:\", X_train_p.shape, \"X_test_p:\", X_test_p.shape)\n",
    "    print(\"Balance train:\", y_train_p.value_counts(normalize=True).round(3).to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f31d4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3.2) Modelos baseline propensi√≥n\n",
    "# ============================================\n",
    "if df_prop is None:\n",
    "    pass\n",
    "else:\n",
    "    models_prop = {\n",
    "        \"dummy_most_frequent\": Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"model\", DummyClassifier(strategy=\"most_frequent\", random_state=RANDOM_STATE))\n",
    "        ]),\n",
    "        \"logreg_balanced\": Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler(with_mean=False)),\n",
    "            (\"model\", LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=RANDOM_STATE))\n",
    "        ]),\n",
    "        \"random_forest\": Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"model\", RandomForestClassifier(\n",
    "                n_estimators=400,\n",
    "                random_state=RANDOM_STATE,\n",
    "                n_jobs=-1,\n",
    "                class_weight=\"balanced_subsample\"\n",
    "            ))\n",
    "        ]),\n",
    "        \"grad_boost\": Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"model\", GradientBoostingClassifier(random_state=RANDOM_STATE))\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "    results_prop = []\n",
    "    fitted_prop = {}\n",
    "\n",
    "    for name, model in models_prop.items():\n",
    "        out = evaluate_binary_classifier(\n",
    "            model=model,\n",
    "            X_train=X_train_p, y_train=y_train_p,\n",
    "            X_test=X_test_p, y_test=y_test_p,\n",
    "            threshold=0.50,\n",
    "            name=name\n",
    "        )\n",
    "        results_prop.append(out[\"metrics\"])\n",
    "        fitted_prop[name] = out\n",
    "\n",
    "    df_results_prop = pd.DataFrame(results_prop).sort_values([\"roc_auc\", \"pr_auc\"], ascending=False)\n",
    "    df_results_prop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1013bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3.3) Umbral + importancia (mejor modelo)\n",
    "# ============================================\n",
    "if df_prop is None:\n",
    "    pass\n",
    "else:\n",
    "    best_prop_model_name = df_results_prop.iloc[0][\"model\"]\n",
    "    best_prop = fitted_prop[best_prop_model_name]\n",
    "    p_test_prop = best_prop[\"proba_test\"]\n",
    "\n",
    "    best_t_prop, sweep_prop = threshold_sweep(y_test_p, p_test_prop, metric=\"f1\")\n",
    "    print(\"Mejor umbral (F1) propensi√≥n:\", best_t_prop)\n",
    "    display(sweep_prop.head(10))\n",
    "\n",
    "    _ = evaluate_binary_classifier(\n",
    "        model=best_prop[\"fitted_model\"],\n",
    "        X_train=X_train_p, y_train=y_train_p,\n",
    "        X_test=X_test_p, y_test=y_test_p,\n",
    "        threshold=best_t_prop,\n",
    "        name=f\"{best_prop_model_name}_tuned\"\n",
    "    )\n",
    "\n",
    "    plot_score_distributions(y_test_p, p_test_prop, title=f\"Distribuci√≥n de scores - {best_prop_model_name}\")\n",
    "\n",
    "    # Importancias\n",
    "    top_imp_prop = safe_feature_importance(best_prop[\"fitted_model\"], feature_names=list(X_train_p.columns), top_n=25)\n",
    "    print(\"Top importancias (si aplica):\")\n",
    "    display(top_imp_prop)\n",
    "\n",
    "    if _HAS_PERM:\n",
    "        perm_prop = permutation_importance_report(best_prop[\"fitted_model\"], X_test_p, y_test_p, top_n=25)\n",
    "        print(\"\\nPermutation importance (top 25):\")\n",
    "        display(perm_prop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c9aa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3.4) Export m√©tricas propensi√≥n + guardado modelo\n",
    "# ============================================\n",
    "if df_prop is None:\n",
    "    pass\n",
    "else:\n",
    "    prop_metrics_path = OUTPUT_MODELS_DIR / \"propension_metrics_baseline.csv\"\n",
    "    df_results_prop.to_csv(prop_metrics_path, index=False, encoding=\"utf-8\")\n",
    "    print(\"‚úÖ Exportado:\", prop_metrics_path)\n",
    "\n",
    "    try:\n",
    "        import joblib\n",
    "        prop_model_path = OUTPUT_MODELS_DIR / f\"propension_model_{best_prop_model_name}.joblib\"\n",
    "        joblib.dump(best_prop[\"fitted_model\"], prop_model_path)\n",
    "        print(\"‚úÖ Modelo guardado:\", prop_model_path)\n",
    "    except Exception as e:\n",
    "        print(\"‚ÑπÔ∏è No se pudo guardar el modelo (joblib). Motivo:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0eaf89",
   "metadata": {},
   "source": [
    "## 4) Siguientes pasos (para el curso)\n",
    "\n",
    "**Ideas de ampliaci√≥n para pr√°cticas:**\n",
    "- Validaci√≥n m√°s realista:\n",
    "  - churn: split temporal (train en meses antiguos, test en meses recientes)\n",
    "  - propensi√≥n: split por cliente + temporal\n",
    "- Tratamiento de desbalance:\n",
    "  - `class_weight`, *undersampling/oversampling*, m√©tricas PR-AUC\n",
    "- Ingenier√≠a de variables adicional:\n",
    "  - features por categor√≠a preferida, tasa de descuento, estacionalidad\n",
    "- Modelos adicionales:\n",
    "  - XGBoost/LightGBM (si se permite en el curso)\n",
    "- Trazabilidad:\n",
    "  - guardar el `threshold` elegido y un diccionario de columnas para scoring en producci√≥n\n",
    "\n",
    "> Importante: en un caso real, los features deben calcularse ‚Äúmirando hacia atr√°s‚Äù (ventanas hist√≥ricas) para evitar **leakage**.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

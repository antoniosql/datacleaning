{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36bbfe82",
   "metadata": {},
   "source": [
    "# Notebook 4 – Integración y Fact Table (FraSoHome)\n",
    "\n",
    "## Objetivo formativo\n",
    "En este laboratorio vamos a **integrar** las distintas fuentes del caso *FraSoHome* (CRM, e-commerce y POS) para construir una **tabla de hechos (fact table)** con granularidad **línea de transacción** (venta o devolución).\n",
    "\n",
    "El foco está en:\n",
    "- Unificar **claves** y **formatos** (IDs, fechas, importes).\n",
    "- Construir una fact table con un **esquema común** para *ONLINE* y *POS*.\n",
    "- Enriquecer la fact con dimensiones (**clientes**, **productos**, **tiendas**).\n",
    "- Ejecutar **comprobaciones básicas** de consistencia tras la integración.\n",
    "- Exportar el resultado para los notebooks posteriores (**features / RFM / churn / basket / propensión**).\n",
    "\n",
    "> Nota: este notebook asume que existen los CSV de origen:\n",
    "> `crm.csv`, `productos.csv`, `tiendas.csv`,\n",
    "> `pedidos.csv`, `lineas_pedido.csv`, `devoluciones_online.csv`,\n",
    "> `ventas_pos.csv`, `devoluciones_tienda.csv`.\n",
    ">\n",
    "> Si has ejecutado el Notebook 3 y generaste `outputs/*_clean.csv`, el notebook intentará cargar primero la versión limpia.\n",
    "> Si no existe, usará la versión raw y aplicará una **estandarización mínima**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203b9bef",
   "metadata": {},
   "source": [
    "## 1) Setup\n",
    "- Leemos CSV como texto (`dtype=str`) para no “romper” por datos sucios.\n",
    "- Creamos funciones reutilizables que aceptan **dataframes como parámetros**.\n",
    "- Generamos columnas nuevas parseadas (`*_dt`, `*_num`, `*_clean`) para no perder el valor original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d04fcb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5154787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Todos los archivos raw esperados existen en DATA_DIR.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\o'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\o'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\o'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\o'\n",
      "C:\\Users\\anton\\AppData\\Local\\Temp\\ipykernel_29008\\3546707504.py:4: SyntaxWarning: invalid escape sequence '\\o'\n",
      "  CLEAN_DIR = Path(\"notebooks\\outputs\" )  # salida del Notebook 3 (si existe)\n",
      "C:\\Users\\anton\\AppData\\Local\\Temp\\ipykernel_29008\\3546707504.py:5: SyntaxWarning: invalid escape sequence '\\o'\n",
      "  OUT_DIR = Path(\"..\\outputs\")\n"
     ]
    }
   ],
   "source": [
    "# --- Configuración de rutas ---\n",
    "# Ajusta DATA_DIR a la carpeta donde tienes los CSV.\n",
    "DATA_DIR = Path(r\"..\\data\\raw\"\"\") # por defecto: carpeta del notebook\n",
    "CLEAN_DIR = Path(\"notebooks\\outputs\" )  # salida del Notebook 3 (si existe)\n",
    "OUT_DIR = Path(\"..\\outputs\")\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "EXPECTED_RAW_FILES = [\n",
    "    \"crm.csv\",\n",
    "    \"productos.csv\",\n",
    "    \"tiendas.csv\",\n",
    "    \"pedidos.csv\",\n",
    "    \"lineas_pedido.csv\",\n",
    "    \"devoluciones_online.csv\",\n",
    "    \"ventas_pos.csv\",\n",
    "    \"devoluciones_tienda.csv\",\n",
    "]\n",
    "\n",
    "def check_expected_files(data_dir: Path, files: List[str]) -> None:\n",
    "    missing = [f for f in files if not (data_dir / f).exists()]\n",
    "    if missing:\n",
    "        print(\"[WARN] Faltan archivos esperados en DATA_DIR:\")\n",
    "        for f in missing:\n",
    "            print(\" -\", f)\n",
    "        print(\"Ajusta DATA_DIR o copia los archivos a la carpeta del notebook.\")\n",
    "    else:\n",
    "        print(\"[OK] Todos los archivos raw esperados existen en DATA_DIR.\")\n",
    "\n",
    "check_expected_files(DATA_DIR, EXPECTED_RAW_FILES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57333e08",
   "metadata": {},
   "source": [
    "## 2) Funciones utilitarias (reutilizables)\n",
    "\n",
    "Helpers para:\n",
    "- **Carga robusta** de CSV (tolerante a líneas raras).\n",
    "- Estandarizar IDs (`strip` + `upper`).\n",
    "- Parseo de **importes** con €, coma/punto, miles, etc.\n",
    "- Parseo de **fechas** heterogéneas (ISO, DD/MM/YY, texto “10 de Enero de 2023”, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "875e92ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_robust(path: Path, **kwargs) -> pd.DataFrame:\n",
    "    # Carga robusta de CSV en UTF-8. Si falla, reintenta con engine='python'\n",
    "    base_kwargs = dict(dtype=str, encoding=\"utf-8\", low_memory=False)\n",
    "    base_kwargs.update(kwargs)\n",
    "\n",
    "    try:\n",
    "        return pd.read_csv(path, **base_kwargs)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Lectura estándar falló en {path.name}: {e}\")\n",
    "        print(\"       Reintentando con engine='python' y on_bad_lines='warn' ...\")\n",
    "        base_kwargs.update(dict(engine=\"python\", on_bad_lines=\"warn\"))\n",
    "        return pd.read_csv(path, **base_kwargs)\n",
    "\n",
    "\n",
    "def standardize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # lower + strip + espacios -> '_'\n",
    "    out = df.copy()\n",
    "    out.columns = (\n",
    "        out.columns.astype(str)\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .str.replace(\" \", \"_\")\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "def standardize_id(series: pd.Series) -> pd.Series:\n",
    "    # strip + quitar espacios internos + upper\n",
    "    s = series.astype(str)\n",
    "    s = s.replace({\"nan\": np.nan, \"None\": np.nan})\n",
    "    s = s.str.strip()\n",
    "    s = s.str.replace(r\"\\s+\", \"\", regex=True)\n",
    "    s = s.str.upper()\n",
    "    s = s.replace({\"\": np.nan})\n",
    "    return s\n",
    "\n",
    "\n",
    "def _normalize_number_str(x: Optional[str]) -> Optional[str]:\n",
    "    if x is None:\n",
    "        return None\n",
    "    x = str(x).strip()\n",
    "    if x == \"\" or x.lower() in {\"nan\", \"none\"}:\n",
    "        return None\n",
    "\n",
    "    # deja solo dígitos, separadores y signo\n",
    "    x = re.sub(r\"[^\\d,.\\-]\", \"\", x)\n",
    "\n",
    "    # múltiples comas sin puntos: 15,7,00 -> 157.00 (última coma decimal)\n",
    "    if x.count(\",\") > 1 and x.count(\".\") == 0:\n",
    "        parts = x.split(\",\")\n",
    "        x = \"\".join(parts[:-1]) + \".\" + parts[-1]\n",
    "\n",
    "    # múltiples puntos sin comas: 1.234.56 -> 1234.56 (último punto decimal)\n",
    "    if x.count(\".\") > 1 and x.count(\",\") == 0:\n",
    "        parts = x.split(\".\")\n",
    "        x = \"\".join(parts[:-1]) + \".\" + parts[-1]\n",
    "\n",
    "    # si contiene punto y coma: decidir separador decimal por el último separador\n",
    "    if (\",\" in x) and (\".\" in x):\n",
    "        if x.rfind(\",\") > x.rfind(\".\"):\n",
    "            # coma decimal -> quitar puntos de miles\n",
    "            x = x.replace(\".\", \"\")\n",
    "            x = x.replace(\",\", \".\")\n",
    "        else:\n",
    "            # punto decimal -> quitar comas de miles\n",
    "            x = x.replace(\",\", \"\")\n",
    "    else:\n",
    "        # solo coma -> coma decimal\n",
    "        if \",\" in x and \".\" not in x:\n",
    "            x = x.replace(\",\", \".\")\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def parse_numeric_mixed(series: pd.Series) -> pd.Series:\n",
    "    s = series.copy()\n",
    "    s = s.where(s.notna(), None)\n",
    "    normalized = s.apply(_normalize_number_str)\n",
    "    return pd.to_numeric(normalized, errors=\"coerce\")\n",
    "\n",
    "\n",
    "SPANISH_MONTHS = {\n",
    "    \"enero\": \"01\", \"febrero\": \"02\", \"marzo\": \"03\", \"abril\": \"04\",\n",
    "    \"mayo\": \"05\", \"junio\": \"06\", \"julio\": \"07\", \"agosto\": \"08\",\n",
    "    \"septiembre\": \"09\", \"setiembre\": \"09\", \"octubre\": \"10\", \"noviembre\": \"11\", \"diciembre\": \"12\",\n",
    "}\n",
    "\n",
    "def _normalize_spanish_date_text(s: str) -> str:\n",
    "    # Convierte '10 de Enero de 2023' (y variantes) a '10/01/2023' conservando hora si existe.\n",
    "    s0 = s.strip()\n",
    "    s0_low = s0.lower()\n",
    "\n",
    "    m = re.search(r\"(\\d{1,2})\\s*de\\s*([a-záéíóúñ]+)\\s*de\\s*(\\d{4})(.*)$\", s0_low)\n",
    "    if not m:\n",
    "        return s0\n",
    "\n",
    "    day = m.group(1)\n",
    "    month_txt = m.group(2)\n",
    "    year = m.group(3)\n",
    "    rest = m.group(4)\n",
    "\n",
    "    month_txt_norm = (\n",
    "        month_txt.replace(\"á\",\"a\").replace(\"é\",\"e\").replace(\"í\",\"i\")\n",
    "                 .replace(\"ó\",\"o\").replace(\"ú\",\"u\")\n",
    "    )\n",
    "\n",
    "    month_num = SPANISH_MONTHS.get(month_txt_norm, None)\n",
    "    if month_num is None:\n",
    "        return s0\n",
    "\n",
    "    return f\"{day}/{month_num}/{year}{rest}\"\n",
    "\n",
    "\n",
    "def parse_datetime_es(series: pd.Series, dayfirst: bool = True) -> pd.Series:\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = s.replace({\"\": np.nan, \"nan\": np.nan, \"None\": np.nan})\n",
    "\n",
    "    dt = pd.to_datetime(s, errors=\"coerce\", dayfirst=dayfirst, infer_datetime_format=True)\n",
    "\n",
    "    mask = dt.isna() & s.notna()\n",
    "    if mask.any():\n",
    "        s2 = s[mask].apply(_normalize_spanish_date_text)\n",
    "        dt2 = pd.to_datetime(s2, errors=\"coerce\", dayfirst=dayfirst, infer_datetime_format=True)\n",
    "        dt.loc[mask] = dt2\n",
    "\n",
    "    return dt\n",
    "\n",
    "\n",
    "def coalesce(*series_list: pd.Series) -> pd.Series:\n",
    "    out = None\n",
    "    for s in series_list:\n",
    "        if out is None:\n",
    "            out = s.copy()\n",
    "        else:\n",
    "            out = out.combine_first(s)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6183b808",
   "metadata": {},
   "source": [
    "## 3) Carga de datasets (raw vs clean)\n",
    "\n",
    "Definimos un cargador:\n",
    "- Si existe `output_clean/<nombre>_clean.csv`, lo usa.\n",
    "- Si no existe, carga el raw en `DATA_DIR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c3e9bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD] crm: usando RAW -> ..\\data\\raw\\crm.csv\n",
      "[LOAD] productos: usando RAW -> ..\\data\\raw\\productos.csv\n",
      "[LOAD] tiendas: usando RAW -> ..\\data\\raw\\tiendas.csv\n",
      "[LOAD] pedidos: usando RAW -> ..\\data\\raw\\pedidos.csv\n",
      "[LOAD] lineas_pedido: usando RAW -> ..\\data\\raw\\lineas_pedido.csv\n",
      "[LOAD] devoluciones_online: usando RAW -> ..\\data\\raw\\devoluciones_online.csv\n",
      "[LOAD] ventas_pos: usando RAW -> ..\\data\\raw\\ventas_pos.csv\n",
      "[LOAD] devoluciones_tienda: usando RAW -> ..\\data\\raw\\devoluciones_tienda.csv\n",
      "\n",
      "Shapes cargados:\n",
      "- crm           : (103, 21)\n",
      "- productos     : (101, 19)\n",
      "- tiendas       : (8, 20)\n",
      "- pedidos       : (656, 17)\n",
      "- lineas        : (1949, 13)\n",
      "- devol_online  : (224, 13)\n",
      "- ventas_pos    : (2521, 20)\n",
      "- devol_pos     : (211, 17)\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(name: str, raw_filename: str, clean_filename: Optional[str] = None) -> pd.DataFrame:\n",
    "    if clean_filename is None:\n",
    "        clean_filename = f\"{name}_clean.csv\"\n",
    "\n",
    "    clean_path = CLEAN_DIR / clean_filename\n",
    "    raw_path = DATA_DIR / raw_filename\n",
    "\n",
    "    if clean_path.exists():\n",
    "        print(f\"[LOAD] {name}: usando CLEAN -> {clean_path}\")\n",
    "        df = load_csv_robust(clean_path)\n",
    "        df[\"_source_loaded_from\"] = \"clean\"\n",
    "    else:\n",
    "        print(f\"[LOAD] {name}: usando RAW -> {raw_path}\")\n",
    "        df = load_csv_robust(raw_path)\n",
    "        df[\"_source_loaded_from\"] = \"raw\"\n",
    "\n",
    "    df = standardize_columns(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "crm = load_dataset(\"crm\", \"crm.csv\", \"crm_clean.csv\")\n",
    "productos = load_dataset(\"productos\", \"productos.csv\", \"productos_clean.csv\")\n",
    "tiendas = load_dataset(\"tiendas\", \"tiendas.csv\", \"tiendas_clean.csv\")\n",
    "\n",
    "pedidos = load_dataset(\"pedidos\", \"pedidos.csv\", \"pedidos_clean.csv\")\n",
    "lineas = load_dataset(\"lineas_pedido\", \"lineas_pedido.csv\", \"lineas_pedido_clean.csv\")\n",
    "devol_online = load_dataset(\"devoluciones_online\", \"devoluciones_online.csv\", \"devoluciones_online_clean.csv\")\n",
    "\n",
    "ventas_pos = load_dataset(\"ventas_pos\", \"ventas_pos.csv\", \"ventas_pos_clean.csv\")\n",
    "devol_pos = load_dataset(\"devoluciones_tienda\", \"devoluciones_tienda.csv\", \"devoluciones_tienda_clean.csv\")\n",
    "\n",
    "print(\"\\nShapes cargados:\")\n",
    "for name, df in {\n",
    "    \"crm\": crm,\n",
    "    \"productos\": productos,\n",
    "    \"tiendas\": tiendas,\n",
    "    \"pedidos\": pedidos,\n",
    "    \"lineas\": lineas,\n",
    "    \"devol_online\": devol_online,\n",
    "    \"ventas_pos\": ventas_pos,\n",
    "    \"devol_pos\": devol_pos,\n",
    "}.items():\n",
    "    print(f\"- {name:14s}: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d530a0c4",
   "metadata": {},
   "source": [
    "## 4) Preparación mínima (IDs, fechas, importes) por dataset\n",
    "\n",
    "Creamos funciones por tabla para:\n",
    "- estandarizar IDs (`*_clean`)\n",
    "- parsear fechas (`*_dt`)\n",
    "- parsear importes/cantidades (`*_num`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dbaf5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Preparación mínima aplicada.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\AppData\\Local\\Temp\\ipykernel_29008\\3031435968.py:118: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dt = pd.to_datetime(s, errors=\"coerce\", dayfirst=dayfirst, infer_datetime_format=True)\n",
      "C:\\Users\\anton\\AppData\\Local\\Temp\\ipykernel_29008\\3031435968.py:123: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dt2 = pd.to_datetime(s2, errors=\"coerce\", dayfirst=dayfirst, infer_datetime_format=True)\n",
      "C:\\Users\\anton\\AppData\\Local\\Temp\\ipykernel_29008\\3031435968.py:123: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  dt2 = pd.to_datetime(s2, errors=\"coerce\", dayfirst=dayfirst, infer_datetime_format=True)\n",
      "C:\\Users\\anton\\AppData\\Local\\Temp\\ipykernel_29008\\3031435968.py:118: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dt = pd.to_datetime(s, errors=\"coerce\", dayfirst=dayfirst, infer_datetime_format=True)\n",
      "C:\\Users\\anton\\AppData\\Local\\Temp\\ipykernel_29008\\3031435968.py:118: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  dt = pd.to_datetime(s, errors=\"coerce\", dayfirst=dayfirst, infer_datetime_format=True)\n",
      "C:\\Users\\anton\\AppData\\Local\\Temp\\ipykernel_29008\\3031435968.py:123: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dt2 = pd.to_datetime(s2, errors=\"coerce\", dayfirst=dayfirst, infer_datetime_format=True)\n",
      "C:\\Users\\anton\\AppData\\Local\\Temp\\ipykernel_29008\\3031435968.py:118: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dt = pd.to_datetime(s, errors=\"coerce\", dayfirst=dayfirst, infer_datetime_format=True)\n",
      "C:\\Users\\anton\\AppData\\Local\\Temp\\ipykernel_29008\\3031435968.py:118: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  dt = pd.to_datetime(s, errors=\"coerce\", dayfirst=dayfirst, infer_datetime_format=True)\n",
      "C:\\Users\\anton\\AppData\\Local\\Temp\\ipykernel_29008\\3031435968.py:123: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dt2 = pd.to_datetime(s2, errors=\"coerce\", dayfirst=dayfirst, infer_datetime_format=True)\n",
      "C:\\Users\\anton\\AppData\\Local\\Temp\\ipykernel_29008\\3031435968.py:118: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dt = pd.to_datetime(s, errors=\"coerce\", dayfirst=dayfirst, infer_datetime_format=True)\n",
      "C:\\Users\\anton\\AppData\\Local\\Temp\\ipykernel_29008\\3031435968.py:118: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  dt = pd.to_datetime(s, errors=\"coerce\", dayfirst=dayfirst, infer_datetime_format=True)\n",
      "C:\\Users\\anton\\AppData\\Local\\Temp\\ipykernel_29008\\3031435968.py:123: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dt2 = pd.to_datetime(s2, errors=\"coerce\", dayfirst=dayfirst, infer_datetime_format=True)\n",
      "C:\\Users\\anton\\AppData\\Local\\Temp\\ipykernel_29008\\3031435968.py:118: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dt = pd.to_datetime(s, errors=\"coerce\", dayfirst=dayfirst, infer_datetime_format=True)\n",
      "C:\\Users\\anton\\AppData\\Local\\Temp\\ipykernel_29008\\3031435968.py:123: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dt2 = pd.to_datetime(s2, errors=\"coerce\", dayfirst=dayfirst, infer_datetime_format=True)\n",
      "C:\\Users\\anton\\AppData\\Local\\Temp\\ipykernel_29008\\3031435968.py:118: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dt = pd.to_datetime(s, errors=\"coerce\", dayfirst=dayfirst, infer_datetime_format=True)\n",
      "C:\\Users\\anton\\AppData\\Local\\Temp\\ipykernel_29008\\3031435968.py:123: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dt2 = pd.to_datetime(s2, errors=\"coerce\", dayfirst=dayfirst, infer_datetime_format=True)\n",
      "C:\\Users\\anton\\AppData\\Local\\Temp\\ipykernel_29008\\3031435968.py:123: UserWarning: Parsing dates in %Y-%m-%d %H:%M:%S format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  dt2 = pd.to_datetime(s2, errors=\"coerce\", dayfirst=dayfirst, infer_datetime_format=True)\n",
      "C:\\Users\\anton\\AppData\\Local\\Temp\\ipykernel_29008\\3031435968.py:118: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dt = pd.to_datetime(s, errors=\"coerce\", dayfirst=dayfirst, infer_datetime_format=True)\n",
      "C:\\Users\\anton\\AppData\\Local\\Temp\\ipykernel_29008\\3031435968.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  dt = pd.to_datetime(s, errors=\"coerce\", dayfirst=dayfirst, infer_datetime_format=True)\n",
      "C:\\Users\\anton\\AppData\\Local\\Temp\\ipykernel_29008\\3031435968.py:123: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dt2 = pd.to_datetime(s2, errors=\"coerce\", dayfirst=dayfirst, infer_datetime_format=True)\n",
      "C:\\Users\\anton\\AppData\\Local\\Temp\\ipykernel_29008\\3031435968.py:118: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dt = pd.to_datetime(s, errors=\"coerce\", dayfirst=dayfirst, infer_datetime_format=True)\n",
      "C:\\Users\\anton\\AppData\\Local\\Temp\\ipykernel_29008\\3031435968.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  dt = pd.to_datetime(s, errors=\"coerce\", dayfirst=dayfirst, infer_datetime_format=True)\n",
      "C:\\Users\\anton\\AppData\\Local\\Temp\\ipykernel_29008\\3031435968.py:123: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dt2 = pd.to_datetime(s2, errors=\"coerce\", dayfirst=dayfirst, infer_datetime_format=True)\n"
     ]
    }
   ],
   "source": [
    "def prepare_crm(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[\"customer_id_clean\"] = standardize_id(out.get(\"customer_id\"))\n",
    "    out[\"fecha_alta_dt\"] = parse_datetime_es(out.get(\"fecha_alta_programa\"))\n",
    "    out[\"fecha_baja_dt\"] = parse_datetime_es(out.get(\"fecha_baja\"))\n",
    "    out[\"puntos_num\"] = parse_numeric_mixed(out.get(\"puntos_acumulados\"))\n",
    "    return out\n",
    "\n",
    "\n",
    "def prepare_productos(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[\"product_id_clean\"] = standardize_id(out.get(\"product_id\"))\n",
    "    out[\"precio_venta_num\"] = parse_numeric_mixed(out.get(\"precio_venta\"))\n",
    "    out[\"coste_unitario_num\"] = parse_numeric_mixed(out.get(\"coste_unitario\"))\n",
    "    out[\"fecha_alta_catalogo_dt\"] = parse_datetime_es(out.get(\"fecha_alta_catalogo\"))\n",
    "    out[\"iva_pct_num\"] = parse_numeric_mixed(out.get(\"iva_pct\"))\n",
    "    return out\n",
    "\n",
    "\n",
    "def prepare_tiendas(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[\"store_id_clean\"] = standardize_id(out.get(\"store_id\"))\n",
    "    out[\"fecha_apertura_dt\"] = parse_datetime_es(out.get(\"fecha_apertura\"))\n",
    "    out[\"metros_cuadrados_num\"] = parse_numeric_mixed(out.get(\"metros_cuadrados\"))\n",
    "    out[\"lat_num\"] = parse_numeric_mixed(out.get(\"lat\"))\n",
    "    out[\"lon_num\"] = parse_numeric_mixed(out.get(\"lon\"))\n",
    "    return out\n",
    "\n",
    "\n",
    "def prepare_pedidos(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[\"order_id_clean\"] = standardize_id(out.get(\"order_id\"))\n",
    "    out[\"customer_id_clean\"] = standardize_id(out.get(\"customer_id\"))\n",
    "    out[\"fecha_pedido_dt\"] = parse_datetime_es(out.get(\"fecha_pedido\"))\n",
    "    out[\"importe_total_num\"] = parse_numeric_mixed(out.get(\"importe_total\"))\n",
    "    out[\"gastos_envio_num\"] = parse_numeric_mixed(out.get(\"gastos_envio\"))\n",
    "    return out\n",
    "\n",
    "\n",
    "def prepare_lineas(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[\"order_line_id_clean\"] = standardize_id(out.get(\"order_line_id\"))\n",
    "    out[\"order_id_clean\"] = standardize_id(out.get(\"order_id\"))\n",
    "    out[\"product_id_clean\"] = standardize_id(out.get(\"product_id\"))\n",
    "\n",
    "    out[\"cantidad_num\"] = parse_numeric_mixed(out.get(\"cantidad\"))\n",
    "    out[\"precio_unitario_num\"] = parse_numeric_mixed(out.get(\"precio_unitario\"))\n",
    "    out[\"descuento_pct_num\"] = parse_numeric_mixed(out.get(\"descuento_pct\"))\n",
    "    out[\"descuento_importe_num\"] = parse_numeric_mixed(out.get(\"descuento_importe\"))\n",
    "    out[\"importe_linea_num\"] = parse_numeric_mixed(out.get(\"importe_linea\"))\n",
    "    out[\"iva_pct_num\"] = parse_numeric_mixed(out.get(\"iva_pct\"))\n",
    "    return out\n",
    "\n",
    "\n",
    "def prepare_devol_online(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[\"return_id_clean\"] = standardize_id(out.get(\"return_id\"))\n",
    "    out[\"order_id_clean\"] = standardize_id(out.get(\"order_id\"))\n",
    "    out[\"order_line_id_clean\"] = standardize_id(out.get(\"order_line_id\"))\n",
    "    out[\"product_id_clean\"] = standardize_id(out.get(\"product_id\"))\n",
    "    out[\"fecha_devolucion_dt\"] = parse_datetime_es(out.get(\"fecha_devolucion\"))\n",
    "    out[\"cantidad_devuelta_num\"] = parse_numeric_mixed(out.get(\"cantidad_devuelta\"))\n",
    "    out[\"importe_reembolsado_num\"] = parse_numeric_mixed(out.get(\"importe_reembolsado\"))\n",
    "    return out\n",
    "\n",
    "\n",
    "def prepare_ventas_pos(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[\"ticket_line_id_clean\"] = standardize_id(out.get(\"ticket_line_id\"))\n",
    "    out[\"ticket_id_clean\"] = standardize_id(out.get(\"ticket_id\"))\n",
    "    out[\"store_id_clean\"] = standardize_id(out.get(\"store_id\"))\n",
    "    out[\"customer_id_clean\"] = standardize_id(out.get(\"customer_id\"))\n",
    "    out[\"product_id_clean\"] = standardize_id(out.get(\"product_id\"))\n",
    "\n",
    "    out[\"fecha_hora_dt\"] = parse_datetime_es(out.get(\"fecha_hora\"))\n",
    "    out[\"cantidad_num\"] = parse_numeric_mixed(out.get(\"cantidad\"))\n",
    "    out[\"precio_unitario_num\"] = parse_numeric_mixed(out.get(\"precio_unitario\"))\n",
    "    out[\"descuento_pct_num\"] = parse_numeric_mixed(out.get(\"descuento_pct\"))\n",
    "    out[\"descuento_importe_num\"] = parse_numeric_mixed(out.get(\"descuento_importe\"))\n",
    "    out[\"importe_linea_num\"] = parse_numeric_mixed(out.get(\"importe_linea\"))\n",
    "    return out\n",
    "\n",
    "\n",
    "def prepare_devol_pos(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[\"return_id_clean\"] = standardize_id(out.get(\"return_id\"))\n",
    "    out[\"store_id_clean\"] = standardize_id(out.get(\"store_id\"))\n",
    "    out[\"ticket_id_original_clean\"] = standardize_id(out.get(\"ticket_id_original\"))\n",
    "    out[\"ticket_line_id_original_clean\"] = standardize_id(out.get(\"ticket_line_id_original\"))\n",
    "    out[\"order_id_original_clean\"] = standardize_id(out.get(\"order_id_original\"))\n",
    "    out[\"customer_id_clean\"] = standardize_id(out.get(\"customer_id\"))\n",
    "    out[\"product_id_clean\"] = standardize_id(out.get(\"product_id\"))\n",
    "\n",
    "    out[\"fecha_devolucion_dt\"] = parse_datetime_es(out.get(\"fecha_devolucion\"))\n",
    "    out[\"cantidad_devuelta_num\"] = parse_numeric_mixed(out.get(\"cantidad_devuelta\"))\n",
    "    out[\"importe_reembolsado_num\"] = parse_numeric_mixed(out.get(\"importe_reembolsado\"))\n",
    "    return out\n",
    "\n",
    "\n",
    "crm_p = prepare_crm(crm)\n",
    "productos_p = prepare_productos(productos)\n",
    "tiendas_p = prepare_tiendas(tiendas)\n",
    "\n",
    "pedidos_p = prepare_pedidos(pedidos)\n",
    "lineas_p = prepare_lineas(lineas)\n",
    "devol_online_p = prepare_devol_online(devol_online)\n",
    "ventas_pos_p = prepare_ventas_pos(ventas_pos)\n",
    "devol_pos_p = prepare_devol_pos(devol_pos)\n",
    "\n",
    "print(\"[OK] Preparación mínima aplicada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bef3f9b",
   "metadata": {},
   "source": [
    "## 5) Construcción de la Fact Table\n",
    "\n",
    "Construimos 4 bloques y concatenamos:\n",
    "1. Ventas ONLINE = `pedidos` + `lineas_pedido`\n",
    "2. Devoluciones ONLINE = `devoluciones_online` (enlazada con pedidos cuando sea posible)\n",
    "3. Ventas POS = `ventas_pos`\n",
    "4. Devoluciones POS = `devoluciones_tienda` (enlazada con ticket_line original cuando sea posible)\n",
    "\n",
    "Regla de modelado (didáctica):\n",
    "- Para devoluciones, `quantity` y `amount_net` se guardan **en negativo**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0bd203e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 295\u001b[39m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _select_existing(out, FACT_COLS)\n\u001b[32m    294\u001b[39m fact_online_sales = build_fact_online_sales(pedidos_p, lineas_p)\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m fact_online_ret = \u001b[43mbuild_fact_online_returns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevol_online_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpedidos_p\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    296\u001b[39m fact_pos_sales = build_fact_pos_sales(ventas_pos_p)\n\u001b[32m    297\u001b[39m fact_pos_ret = build_fact_pos_returns(devol_pos_p, ventas_pos_p)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 135\u001b[39m, in \u001b[36mbuild_fact_online_returns\u001b[39m\u001b[34m(devol, pedidos)\u001b[39m\n\u001b[32m    132\u001b[39m out[\u001b[33m\"\u001b[39m\u001b[33mstore_id\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mONLINE\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    133\u001b[39m out[\u001b[33m\"\u001b[39m\u001b[33mstore_id_clean\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mONLINE\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m out[\u001b[33m\"\u001b[39m\u001b[33mcustomer_id\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43md\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcustomer_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcombine_first\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcustomer_id_pedido\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m out[\u001b[33m\"\u001b[39m\u001b[33mcustomer_id_clean\u001b[39m\u001b[33m\"\u001b[39m] = d.get(\u001b[33m\"\u001b[39m\u001b[33mcustomer_id_clean\u001b[39m\u001b[33m\"\u001b[39m).combine_first(d.get(\u001b[33m\"\u001b[39m\u001b[33mcustomer_id_clean_pedido\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m    138\u001b[39m out[\u001b[33m\"\u001b[39m\u001b[33mproduct_id\u001b[39m\u001b[33m\"\u001b[39m] = d.get(\u001b[33m\"\u001b[39m\u001b[33mproduct_id\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\core\\series.py:3537\u001b[39m, in \u001b[36mSeries.combine_first\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m   3492\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3493\u001b[39m \u001b[33;03mUpdate null elements with value in the same location in 'other'.\u001b[39;00m\n\u001b[32m   3494\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3533\u001b[39m \u001b[33;03mdtype: float64\u001b[39;00m\n\u001b[32m   3534\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3535\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreshape\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconcat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m concat\n\u001b[32m-> \u001b[39m\u001b[32m3537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dtype == \u001b[43mother\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m:\n\u001b[32m   3538\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.index.equals(other.index):\n\u001b[32m   3539\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mask(\u001b[38;5;28mself\u001b[39m.isna(), other)\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "FACT_COLS = [\n",
    "    \"source_system\", \"source_table\",\n",
    "    \"channel\", \"movement_type\",\n",
    "    \"event_dt\", \"event_dt_raw\",\n",
    "    \"currency\",\n",
    "    \"order_id\", \"order_id_clean\",\n",
    "    \"order_line_id\", \"order_line_id_clean\",\n",
    "    \"ticket_id\", \"ticket_id_clean\",\n",
    "    \"ticket_line_id\", \"ticket_line_id_clean\",\n",
    "    \"return_id\", \"return_id_clean\",\n",
    "    \"store_id\", \"store_id_clean\",\n",
    "    \"customer_id\", \"customer_id_clean\",\n",
    "    \"product_id\", \"product_id_clean\",\n",
    "    \"quantity_raw\", \"quantity\",\n",
    "    \"unit_price_raw\", \"unit_price\",\n",
    "    \"discount_amount_raw\", \"discount_amount\",\n",
    "    \"discount_pct_raw\", \"discount_pct\",\n",
    "    \"amount_net_raw\", \"amount_net\",\n",
    "    \"refund_amount_raw\", \"refund_amount\",\n",
    "    \"status\", \"payment_method\",\n",
    "    \"notes\",\n",
    "]\n",
    "\n",
    "def _select_existing(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:\n",
    "    existing = [c for c in cols if c in df.columns]\n",
    "    out = df[existing].copy()\n",
    "    for c in cols:\n",
    "        if c not in out.columns:\n",
    "            out[c] = np.nan\n",
    "    return out[cols]\n",
    "\n",
    "def build_fact_online_sales(pedidos: pd.DataFrame, lineas: pd.DataFrame) -> pd.DataFrame:\n",
    "    p = pedidos.copy()\n",
    "    l = lineas.copy()\n",
    "\n",
    "    p_small = p[[\n",
    "        \"order_id\", \"order_id_clean\",\n",
    "        \"fecha_pedido\", \"fecha_pedido_dt\",\n",
    "        \"customer_id\", \"customer_id_clean\",\n",
    "        \"moneda\", \"estado_pedido\", \"metodo_pago\"\n",
    "    ]].copy()\n",
    "\n",
    "    merged = l.merge(p_small, on=\"order_id_clean\", how=\"left\", suffixes=(\"_line\", \"_order\"))\n",
    "\n",
    "    out = pd.DataFrame()\n",
    "    out[\"source_system\"] = \"ECOM\"\n",
    "    out[\"source_table\"] = \"lineas_pedido\"\n",
    "    out[\"channel\"] = \"ONLINE\"\n",
    "    out[\"movement_type\"] = \"SALE\"\n",
    "\n",
    "    out[\"event_dt\"] = merged[\"fecha_pedido_dt\"]\n",
    "    out[\"event_dt_raw\"] = merged[\"fecha_pedido\"]\n",
    "    out[\"currency\"] = merged.get(\"moneda\")\n",
    "\n",
    "    out[\"order_id\"] = merged[\"order_id_order\"]\n",
    "    out[\"order_id_clean\"] = merged[\"order_id_clean\"]\n",
    "    out[\"order_line_id\"] = merged.get(\"order_line_id\")\n",
    "    out[\"order_line_id_clean\"] = merged.get(\"order_line_id_clean\")\n",
    "\n",
    "    out[\"ticket_id\"] = np.nan\n",
    "    out[\"ticket_id_clean\"] = np.nan\n",
    "    out[\"ticket_line_id\"] = np.nan\n",
    "    out[\"ticket_line_id_clean\"] = np.nan\n",
    "    out[\"return_id\"] = np.nan\n",
    "    out[\"return_id_clean\"] = np.nan\n",
    "\n",
    "    out[\"store_id\"] = \"ONLINE\"\n",
    "    out[\"store_id_clean\"] = \"ONLINE\"\n",
    "\n",
    "    out[\"customer_id\"] = merged.get(\"customer_id\")\n",
    "    out[\"customer_id_clean\"] = merged.get(\"customer_id_clean\")\n",
    "\n",
    "    out[\"product_id\"] = merged.get(\"product_id\")\n",
    "    out[\"product_id_clean\"] = merged.get(\"product_id_clean\")\n",
    "\n",
    "    out[\"quantity_raw\"] = merged.get(\"cantidad\")\n",
    "    out[\"quantity\"] = merged.get(\"cantidad_num\")\n",
    "\n",
    "    out[\"unit_price_raw\"] = merged.get(\"precio_unitario\")\n",
    "    out[\"unit_price\"] = merged.get(\"precio_unitario_num\")\n",
    "\n",
    "    out[\"discount_amount_raw\"] = merged.get(\"descuento_importe\")\n",
    "    out[\"discount_amount\"] = merged.get(\"descuento_importe_num\")\n",
    "\n",
    "    out[\"discount_pct_raw\"] = merged.get(\"descuento_pct\")\n",
    "    out[\"discount_pct\"] = merged.get(\"descuento_pct_num\")\n",
    "\n",
    "    out[\"amount_net_raw\"] = merged.get(\"importe_linea\")\n",
    "    out[\"amount_net\"] = merged.get(\"importe_linea_num\")\n",
    "\n",
    "    calc_net = (out[\"quantity\"] * out[\"unit_price\"]) - out[\"discount_amount\"].fillna(0)\n",
    "    out[\"amount_net\"] = out[\"amount_net\"].fillna(calc_net)\n",
    "\n",
    "    out[\"refund_amount_raw\"] = np.nan\n",
    "    out[\"refund_amount\"] = np.nan\n",
    "\n",
    "    out[\"status\"] = merged.get(\"estado_pedido\")\n",
    "    out[\"payment_method\"] = merged.get(\"metodo_pago\")\n",
    "    out[\"notes\"] = merged.get(\"_source_loaded_from\")\n",
    "\n",
    "    return _select_existing(out, FACT_COLS)\n",
    "\n",
    "def build_fact_online_returns(devol: pd.DataFrame, pedidos: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = devol.copy()\n",
    "\n",
    "    p_small = pedidos[[\"order_id_clean\", \"customer_id\", \"customer_id_clean\", \"moneda\"]].copy()\n",
    "    d = d.merge(p_small, on=\"order_id_clean\", how=\"left\", suffixes=(\"\", \"_pedido\"))\n",
    "\n",
    "    out = pd.DataFrame()\n",
    "    out[\"source_system\"] = \"ECOM\"\n",
    "    out[\"source_table\"] = \"devoluciones_online\"\n",
    "    out[\"channel\"] = \"ONLINE\"\n",
    "    out[\"movement_type\"] = \"RETURN\"\n",
    "\n",
    "    out[\"event_dt\"] = d.get(\"fecha_devolucion_dt\")\n",
    "    out[\"event_dt_raw\"] = d.get(\"fecha_devolucion\")\n",
    "    out[\"currency\"] = d.get(\"moneda\").combine_first(d.get(\"moneda_pedido\"))\n",
    "\n",
    "    out[\"order_id\"] = d.get(\"order_id\")\n",
    "    out[\"order_id_clean\"] = d.get(\"order_id_clean\")\n",
    "    out[\"order_line_id\"] = d.get(\"order_line_id\")\n",
    "    out[\"order_line_id_clean\"] = d.get(\"order_line_id_clean\")\n",
    "\n",
    "    out[\"ticket_id\"] = np.nan\n",
    "    out[\"ticket_id_clean\"] = np.nan\n",
    "    out[\"ticket_line_id\"] = np.nan\n",
    "    out[\"ticket_line_id_clean\"] = np.nan\n",
    "\n",
    "    out[\"return_id\"] = d.get(\"return_id\")\n",
    "    out[\"return_id_clean\"] = d.get(\"return_id_clean\")\n",
    "\n",
    "    out[\"store_id\"] = \"ONLINE\"\n",
    "    out[\"store_id_clean\"] = \"ONLINE\"\n",
    "\n",
    "    out[\"customer_id\"] = d.get(\"customer_id\").combine_first(d.get(\"customer_id_pedido\"))\n",
    "    out[\"customer_id_clean\"] = d.get(\"customer_id_clean\").combine_first(d.get(\"customer_id_clean_pedido\"))\n",
    "\n",
    "    out[\"product_id\"] = d.get(\"product_id\")\n",
    "    out[\"product_id_clean\"] = d.get(\"product_id_clean\")\n",
    "\n",
    "    out[\"quantity_raw\"] = d.get(\"cantidad_devuelta\")\n",
    "    out[\"quantity\"] = -1 * d.get(\"cantidad_devuelta_num\")\n",
    "\n",
    "    out[\"refund_amount_raw\"] = d.get(\"importe_reembolsado\")\n",
    "    out[\"refund_amount\"] = d.get(\"importe_reembolsado_num\")\n",
    "\n",
    "    out[\"amount_net_raw\"] = d.get(\"importe_reembolsado\")\n",
    "    out[\"amount_net\"] = -1 * d.get(\"importe_reembolsado_num\")\n",
    "\n",
    "    out[\"unit_price_raw\"] = np.nan\n",
    "    out[\"unit_price\"] = np.nan\n",
    "    out[\"discount_amount_raw\"] = np.nan\n",
    "    out[\"discount_amount\"] = np.nan\n",
    "    out[\"discount_pct_raw\"] = np.nan\n",
    "    out[\"discount_pct\"] = np.nan\n",
    "\n",
    "    out[\"status\"] = d.get(\"estado_devolucion\")\n",
    "    out[\"payment_method\"] = d.get(\"metodo_devolucion\")\n",
    "    out[\"notes\"] = d.get(\"motivo_devolucion\")\n",
    "\n",
    "    return _select_existing(out, FACT_COLS)\n",
    "\n",
    "def build_fact_pos_sales(ventas: pd.DataFrame) -> pd.DataFrame:\n",
    "    v = ventas.copy()\n",
    "\n",
    "    out = pd.DataFrame()\n",
    "    out[\"source_system\"] = \"POS\"\n",
    "    out[\"source_table\"] = \"ventas_pos\"\n",
    "    out[\"channel\"] = \"POS\"\n",
    "    out[\"movement_type\"] = \"SALE\"\n",
    "\n",
    "    out[\"event_dt\"] = v.get(\"fecha_hora_dt\")\n",
    "    out[\"event_dt_raw\"] = v.get(\"fecha_hora\")\n",
    "    out[\"currency\"] = v.get(\"moneda\")\n",
    "\n",
    "    out[\"ticket_id\"] = v.get(\"ticket_id\")\n",
    "    out[\"ticket_id_clean\"] = v.get(\"ticket_id_clean\")\n",
    "    out[\"ticket_line_id\"] = v.get(\"ticket_line_id\")\n",
    "    out[\"ticket_line_id_clean\"] = v.get(\"ticket_line_id_clean\")\n",
    "\n",
    "    out[\"order_id\"] = np.nan\n",
    "    out[\"order_id_clean\"] = np.nan\n",
    "    out[\"order_line_id\"] = np.nan\n",
    "    out[\"order_line_id_clean\"] = np.nan\n",
    "\n",
    "    out[\"return_id\"] = np.nan\n",
    "    out[\"return_id_clean\"] = np.nan\n",
    "\n",
    "    out[\"store_id\"] = v.get(\"store_id\")\n",
    "    out[\"store_id_clean\"] = v.get(\"store_id_clean\")\n",
    "\n",
    "    out[\"customer_id\"] = v.get(\"customer_id\")\n",
    "    out[\"customer_id_clean\"] = v.get(\"customer_id_clean\")\n",
    "\n",
    "    out[\"product_id\"] = v.get(\"product_id\")\n",
    "    out[\"product_id_clean\"] = v.get(\"product_id_clean\")\n",
    "\n",
    "    out[\"quantity_raw\"] = v.get(\"cantidad\")\n",
    "    out[\"quantity\"] = v.get(\"cantidad_num\")\n",
    "\n",
    "    out[\"unit_price_raw\"] = v.get(\"precio_unitario\")\n",
    "    out[\"unit_price\"] = v.get(\"precio_unitario_num\")\n",
    "\n",
    "    out[\"discount_amount_raw\"] = v.get(\"descuento_importe\")\n",
    "    out[\"discount_amount\"] = v.get(\"descuento_importe_num\")\n",
    "\n",
    "    out[\"discount_pct_raw\"] = v.get(\"descuento_pct\")\n",
    "    out[\"discount_pct\"] = v.get(\"descuento_pct_num\")\n",
    "\n",
    "    out[\"amount_net_raw\"] = v.get(\"importe_linea\")\n",
    "    out[\"amount_net\"] = v.get(\"importe_linea_num\")\n",
    "\n",
    "    calc_net = (out[\"quantity\"] * out[\"unit_price\"]) - out[\"discount_amount\"].fillna(0)\n",
    "    out[\"amount_net\"] = out[\"amount_net\"].fillna(calc_net)\n",
    "\n",
    "    out[\"refund_amount_raw\"] = np.nan\n",
    "    out[\"refund_amount\"] = np.nan\n",
    "\n",
    "    out[\"status\"] = \"OK\"\n",
    "    out[\"payment_method\"] = np.nan\n",
    "    out[\"notes\"] = v.get(\"observaciones\")\n",
    "\n",
    "    return _select_existing(out, FACT_COLS)\n",
    "\n",
    "def build_fact_pos_returns(devol: pd.DataFrame, ventas_pos: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = devol.copy()\n",
    "\n",
    "    v_small = ventas_pos[[\n",
    "        \"ticket_line_id_clean\", \"ticket_id_clean\",\n",
    "        \"precio_unitario_num\", \"descuento_importe_num\", \"descuento_pct_num\",\n",
    "        \"importe_linea_num\"\n",
    "    ]].copy()\n",
    "\n",
    "    d = d.merge(v_small, left_on=\"ticket_line_id_original_clean\", right_on=\"ticket_line_id_clean\",\n",
    "                how=\"left\", suffixes=(\"\", \"_orig\"))\n",
    "\n",
    "    out = pd.DataFrame()\n",
    "    out[\"source_system\"] = \"POS\"\n",
    "    out[\"source_table\"] = \"devoluciones_tienda\"\n",
    "    out[\"channel\"] = \"POS\"\n",
    "    out[\"movement_type\"] = \"RETURN\"\n",
    "\n",
    "    out[\"event_dt\"] = d.get(\"fecha_devolucion_dt\")\n",
    "    out[\"event_dt_raw\"] = d.get(\"fecha_devolucion\")\n",
    "    out[\"currency\"] = d.get(\"moneda\")\n",
    "\n",
    "    out[\"ticket_id\"] = d.get(\"ticket_id_original\")\n",
    "    out[\"ticket_id_clean\"] = d.get(\"ticket_id_original_clean\")\n",
    "    out[\"ticket_line_id\"] = d.get(\"ticket_line_id_original\")\n",
    "    out[\"ticket_line_id_clean\"] = d.get(\"ticket_line_id_original_clean\")\n",
    "\n",
    "    out[\"order_id\"] = d.get(\"order_id_original\")\n",
    "    out[\"order_id_clean\"] = d.get(\"order_id_original_clean\")\n",
    "    out[\"order_line_id\"] = np.nan\n",
    "    out[\"order_line_id_clean\"] = np.nan\n",
    "\n",
    "    out[\"return_id\"] = d.get(\"return_id\")\n",
    "    out[\"return_id_clean\"] = d.get(\"return_id_clean\")\n",
    "\n",
    "    out[\"store_id\"] = d.get(\"store_id\")\n",
    "    out[\"store_id_clean\"] = d.get(\"store_id_clean\")\n",
    "\n",
    "    out[\"customer_id\"] = d.get(\"customer_id\")\n",
    "    out[\"customer_id_clean\"] = d.get(\"customer_id_clean\")\n",
    "\n",
    "    out[\"product_id\"] = d.get(\"product_id\")\n",
    "    out[\"product_id_clean\"] = d.get(\"product_id_clean\")\n",
    "\n",
    "    out[\"quantity_raw\"] = d.get(\"cantidad_devuelta\")\n",
    "    out[\"quantity\"] = -1 * d.get(\"cantidad_devuelta_num\")\n",
    "\n",
    "    out[\"refund_amount_raw\"] = d.get(\"importe_reembolsado\")\n",
    "    out[\"refund_amount\"] = d.get(\"importe_reembolsado_num\")\n",
    "\n",
    "    out[\"amount_net_raw\"] = d.get(\"importe_reembolsado\")\n",
    "    out[\"amount_net\"] = -1 * d.get(\"importe_reembolsado_num\")\n",
    "\n",
    "    # info de la venta original si existe\n",
    "    out[\"unit_price_raw\"] = np.nan\n",
    "    out[\"unit_price\"] = d.get(\"precio_unitario_num\")\n",
    "\n",
    "    out[\"discount_amount_raw\"] = np.nan\n",
    "    out[\"discount_amount\"] = d.get(\"descuento_importe_num\")\n",
    "\n",
    "    out[\"discount_pct_raw\"] = np.nan\n",
    "    out[\"discount_pct\"] = d.get(\"descuento_pct_num\")\n",
    "\n",
    "    out[\"status\"] = d.get(\"estado_devolucion\")\n",
    "    out[\"payment_method\"] = d.get(\"metodo_reembolso\")\n",
    "    out[\"notes\"] = coalesce(d.get(\"motivo_devolucion\"), d.get(\"canal_origen_venta\"))\n",
    "\n",
    "    return _select_existing(out, FACT_COLS)\n",
    "\n",
    "fact_online_sales = build_fact_online_sales(pedidos_p, lineas_p)\n",
    "fact_online_ret = build_fact_online_returns(devol_online_p, pedidos_p)\n",
    "fact_pos_sales = build_fact_pos_sales(ventas_pos_p)\n",
    "fact_pos_ret = build_fact_pos_returns(devol_pos_p, ventas_pos_p)\n",
    "\n",
    "fact_all = pd.concat([fact_online_sales, fact_online_ret, fact_pos_sales, fact_pos_ret], ignore_index=True)\n",
    "\n",
    "print(\"Filas por bloque:\")\n",
    "print(\" - ONLINE sales :\", len(fact_online_sales))\n",
    "print(\" - ONLINE ret   :\", len(fact_online_ret))\n",
    "print(\" - POS sales    :\", len(fact_pos_sales))\n",
    "print(\" - POS ret      :\", len(fact_pos_ret))\n",
    "print(\"\\nTOTAL fact_all :\", len(fact_all))\n",
    "\n",
    "fact_all.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d509278",
   "metadata": {},
   "source": [
    "## 6) Enriquecimiento con dimensiones (clientes, productos, tiendas)\n",
    "\n",
    "- Productos: categoría, subcategoría, marca, coste unitario, precio lista.\n",
    "- Clientes: tier, puntos, estado, consentimiento marketing.\n",
    "- Tiendas: ciudad, región, tipo (ONLINE vs física), etc.\n",
    "\n",
    "En datasets con errores es normal que existan **huérfanos** (IDs en fact que no aparezcan en dimensiones).\n",
    "Luego mediremos ese %."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ebd62ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fact_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     32\u001b[39m     t_small = t[[c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cols_keep \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m t.columns]].copy()\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fact.merge(t_small, on=\u001b[33m\"\u001b[39m\u001b[33mstore_id_clean\u001b[39m\u001b[33m\"\u001b[39m, how=\u001b[33m\"\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m fact_enriched = \u001b[43mfact_all\u001b[49m.copy()\n\u001b[32m     36\u001b[39m fact_enriched = enrich_with_product_dim(fact_enriched, productos_p)\n\u001b[32m     37\u001b[39m fact_enriched = enrich_with_customer_dim(fact_enriched, crm_p)\n",
      "\u001b[31mNameError\u001b[39m: name 'fact_all' is not defined"
     ]
    }
   ],
   "source": [
    "def enrich_with_product_dim(fact: pd.DataFrame, productos: pd.DataFrame) -> pd.DataFrame:\n",
    "    p = productos.copy()\n",
    "    p = p.sort_values(by=[\"product_id_clean\"]).drop_duplicates(subset=[\"product_id_clean\"], keep=\"first\")\n",
    "\n",
    "    cols_keep = [\n",
    "        \"product_id_clean\", \"nombre_producto\", \"categoria\", \"subcategoria\", \"marca\",\n",
    "        \"precio_venta_num\", \"coste_unitario_num\", \"estado_producto\"\n",
    "    ]\n",
    "    p_small = p[[c for c in cols_keep if c in p.columns]].copy()\n",
    "    return fact.merge(p_small, on=\"product_id_clean\", how=\"left\")\n",
    "\n",
    "def enrich_with_customer_dim(fact: pd.DataFrame, crm: pd.DataFrame) -> pd.DataFrame:\n",
    "    c = crm.copy()\n",
    "    c = c.sort_values(by=[\"customer_id_clean\"]).drop_duplicates(subset=[\"customer_id_clean\"], keep=\"first\")\n",
    "\n",
    "    cols_keep = [\n",
    "        \"customer_id_clean\", \"tier_fidelizacion\", \"puntos_num\",\n",
    "        \"consentimiento_marketing\", \"estado_cliente\",\n",
    "        \"ciudad\", \"provincia\", \"codigo_postal\", \"pais\"\n",
    "    ]\n",
    "    c_small = c[[c for c in cols_keep if c in c.columns]].copy()\n",
    "    return fact.merge(c_small, on=\"customer_id_clean\", how=\"left\")\n",
    "\n",
    "def enrich_with_store_dim(fact: pd.DataFrame, tiendas: pd.DataFrame) -> pd.DataFrame:\n",
    "    t = tiendas.copy()\n",
    "    t = t.sort_values(by=[\"store_id_clean\"]).drop_duplicates(subset=[\"store_id_clean\"], keep=\"first\")\n",
    "\n",
    "    cols_keep = [\n",
    "        \"store_id_clean\", \"nombre_tienda\", \"tipo_ubicacion\", \"canal\",\n",
    "        \"ciudad\", \"provincia\", \"region\", \"estado\"\n",
    "    ]\n",
    "    t_small = t[[c for c in cols_keep if c in t.columns]].copy()\n",
    "    return fact.merge(t_small, on=\"store_id_clean\", how=\"left\")\n",
    "\n",
    "fact_enriched = fact_all.copy()\n",
    "fact_enriched = enrich_with_product_dim(fact_enriched, productos_p)\n",
    "fact_enriched = enrich_with_customer_dim(fact_enriched, crm_p)\n",
    "fact_enriched = enrich_with_store_dim(fact_enriched, tiendas_p)\n",
    "\n",
    "fact_enriched.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cf9349",
   "metadata": {},
   "source": [
    "## 7) Métricas derivadas en la fact table\n",
    "\n",
    "Calculamos métricas útiles para análisis posteriores:\n",
    "- `quantity_abs`\n",
    "- `gross_amount` (qty * unit_price) cuando tengamos unit_price\n",
    "- `amount_net` ya viene con signo (ventas +, devoluciones -)\n",
    "- `cost_total_signed` = coste_unitario * quantity (devoluciones queda negativo)\n",
    "- `margin_signed` = amount_net - cost_total_signed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6201f29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_fact_metrics(fact: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = fact.copy()\n",
    "\n",
    "    out[\"quantity_abs\"] = out[\"quantity\"].abs()\n",
    "    out[\"gross_amount\"] = out[\"quantity\"] * out[\"unit_price\"]\n",
    "\n",
    "    out[\"cost_total_signed\"] = out[\"coste_unitario_num\"] * out[\"quantity\"]\n",
    "    out[\"margin_signed\"] = out[\"amount_net\"] - out[\"cost_total_signed\"]\n",
    "\n",
    "    out[\"is_return\"] = out[\"movement_type\"].eq(\"RETURN\")\n",
    "    out[\"event_date\"] = out[\"event_dt\"].dt.date\n",
    "\n",
    "    return out\n",
    "\n",
    "fact_final = add_fact_metrics(fact_enriched)\n",
    "\n",
    "fact_final[[\n",
    "    \"channel\", \"movement_type\", \"event_dt\", \"store_id_clean\", \"customer_id_clean\", \"product_id_clean\",\n",
    "    \"quantity\", \"amount_net\", \"coste_unitario_num\", \"cost_total_signed\", \"margin_signed\"\n",
    "]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de88fa81",
   "metadata": {},
   "source": [
    "## 8) Comprobaciones básicas post-integración (Data Quality)\n",
    "\n",
    "Cuantificamos problemas típicos tras integrar:\n",
    "- % de fechas no parseadas (`event_dt` NaT)\n",
    "- % de productos no encontrados en catálogo\n",
    "- % de tiendas no encontradas\n",
    "- % de customer_id desconocidos (normal en POS por no identificar al cliente)\n",
    "- Duplicados potenciales por claves naturales\n",
    "- Rango de fechas (min/max) y outliers temporales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278f40da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_rate(series: pd.Series) -> float:\n",
    "    return float(series.isna().mean())\n",
    "\n",
    "def coverage_report(fact: pd.DataFrame) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    rows.append((\"event_dt (parsed)\", 1 - null_rate(fact[\"event_dt\"])))\n",
    "    rows.append((\"product match (nombre_producto)\", 1 - null_rate(fact[\"nombre_producto\"])))\n",
    "    rows.append((\"store match (nombre_tienda)\", 1 - null_rate(fact[\"nombre_tienda\"])))\n",
    "    rows.append((\"customer match (tier_fidelizacion)\", 1 - null_rate(fact[\"tier_fidelizacion\"])))\n",
    "    return pd.DataFrame(rows, columns=[\"metric\", \"coverage\"]).sort_values(\"coverage\")\n",
    "\n",
    "def duplicates_by_natural_key(fact: pd.DataFrame) -> pd.DataFrame:\n",
    "    nat = (\n",
    "        fact[\"channel\"].astype(str) + \"|\" +\n",
    "        fact[\"movement_type\"].astype(str) + \"|\" +\n",
    "        coalesce(fact[\"order_line_id_clean\"], fact[\"ticket_line_id_clean\"], fact[\"return_id_clean\"]).astype(str)\n",
    "    )\n",
    "    tmp = fact.copy()\n",
    "    tmp[\"natural_key\"] = nat.replace(\"nan\", np.nan)\n",
    "    dup = tmp[\"natural_key\"].duplicated(keep=False)\n",
    "    return tmp.loc[dup, [\"natural_key\", \"channel\", \"movement_type\", \"order_line_id\", \"ticket_line_id\", \"return_id\"]].sort_values(\"natural_key\")\n",
    "\n",
    "def date_range_report(fact: pd.DataFrame) -> Dict[str, object]:\n",
    "    dt = fact[\"event_dt\"]\n",
    "    return {\n",
    "        \"min_event_dt\": dt.min(),\n",
    "        \"max_event_dt\": dt.max(),\n",
    "        \"pct_event_dt_null\": null_rate(dt),\n",
    "    }\n",
    "\n",
    "print(\"Cobertura (tras joins/parseo):\")\n",
    "display(coverage_report(fact_final))\n",
    "\n",
    "print(\"\\nRango de fechas:\")\n",
    "print(date_range_report(fact_final))\n",
    "\n",
    "print(\"\\nDistribución por canal / tipo movimiento:\")\n",
    "display(\n",
    "    fact_final.groupby([\"channel\", \"movement_type\"], dropna=False)\n",
    "    .size()\n",
    "    .reset_index(name=\"rows\")\n",
    "    .sort_values(\"rows\", ascending=False)\n",
    ")\n",
    "\n",
    "dups = duplicates_by_natural_key(fact_final)\n",
    "print(f\"\\nPosibles duplicados por clave natural: {len(dups)} filas (muestra 10)\")\n",
    "display(dups.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bacd878",
   "metadata": {},
   "source": [
    "## 9) Exportación\n",
    "\n",
    "Exportamos la fact table integrada a:\n",
    "- `output_integrated/fact_transacciones_integrada.csv`\n",
    "\n",
    "Esta salida será la base del Notebook 5 (features: RFM, churn, basket, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2310931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_fact_id(fact: pd.DataFrame, prefix: str = \"F\") -> pd.DataFrame:\n",
    "    out = fact.copy().reset_index(drop=True)\n",
    "    out.insert(0, \"fact_id\", [f\"{prefix}{i:07d}\" for i in range(1, len(out) + 1)])\n",
    "    return out\n",
    "\n",
    "fact_export = add_fact_id(fact_final)\n",
    "\n",
    "out_path = OUT_DIR / \"fact_transacciones_integrada.csv\"\n",
    "fact_export.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"[OK] Exportado:\", out_path)\n",
    "print(\"Shape:\", fact_export.shape)\n",
    "fact_export.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fd36a7",
   "metadata": {},
   "source": [
    "## 10) Siguientes pasos (Notebook 5)\n",
    "\n",
    "Con `fact_transacciones_integrada.csv` ya puedes:\n",
    "- Calcular **RFM** por cliente.\n",
    "- Derivar features de **tasa de devoluciones**, **canal preferido**, **uso de descuentos**, etc.\n",
    "- Ejecutar **Market Basket Analysis** (por `order_id` y `ticket_id`).\n",
    "- Construir un dataset de **propensión** (features + etiqueta) para ML."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# FraSoHome ‚Äì Notebook 6: Preprocesamiento final (escala y codificaci√≥n) ‚Äì ML/BI\n\n## Objetivos formativos\n\nEn este notebook vamos a preparar los datasets de **features** (generados en el Notebook 5) para que est√©n listos para:\n- **Machine Learning**: datos num√©ricos escalados, categ√≥ricas codificadas, sin nulos.\n- **BI**: versionado de datasets consistentes, con un ‚Äúdiccionario‚Äù de variables y comprobaciones b√°sicas.\n\n> Nota did√°ctica: en este caso, **los or√≠genes contienen errores intencionales** (formatos mixtos, valores raros, etc.).  \n> En Notebook 5 ya convertimos gran parte de esos problemas a features agregadas, pero aqu√≠ a√∫n veremos:\n> - n√∫meros en texto con `‚Ç¨`, coma decimal, separador de miles‚Ä¶\n> - flags booleanos heterog√©neos (`S`, `s√≠`, `YES`, `0/1`, etc.)\n> - categor√≠as con variantes (`Bronce` vs `BRONCE`, etc.)\n\n## Entradas esperadas\n\nEl notebook intenta cargar (por orden de preferencia):\n- `output_features/features_clientes.csv`\n- `output_features/features_productos.csv`\n- `output_features/dataset_propension_snapshots.csv` (si existe)\n\nSi no existen, puedes ejecutar primero el Notebook 5.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# ============================================\n# 0) Imports y configuraci√≥n\n# ============================================\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\nimport json\nimport re\nimport warnings\n\nimport numpy as np\nimport pandas as pd\n\nwarnings.filterwarnings(\"ignore\")\n\n# scikit-learn (para pipelines de ML)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\nfrom sklearn.impute import SimpleImputer\n\npd.set_option(\"display.max_columns\", 200)\npd.set_option(\"display.width\", 160)\n\nDATA_DIR = Path(\".\")  # carpeta donde est√© el notebook\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1) Utilidades: carga robusta y normalizaci√≥n ligera\n\nEstrategia:\n1. Leemos CSV como **texto** (`dtype=str`) para preservar formatos raros.\n2. Estandarizamos nombres de columnas.\n3. Parseamos num√©ricos/booleanos de forma *tolerante* (sin ‚Äúromper‚Äù el flujo).\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# ============================================\n# 1) Utilidades de ingesti√≥n / normalizaci√≥n\n# ============================================\n\ndef load_csv_robust(path: Path, encoding: str = \"utf-8\", dtype=str) -> pd.DataFrame:\n    \"\"\"Carga CSV de forma robusta (pensado para data quality con formatos raros).\"\"\"\n    if not path.exists():\n        raise FileNotFoundError(f\"No existe el fichero: {path.resolve()}\")\n    # engine='python' tolera mejor algunos casos raros de comillas\n    df = pd.read_csv(path, encoding=encoding, dtype=dtype, sep=\",\", engine=\"python\")\n    return df\n\n\ndef standardize_column_names(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Estandariza nombres de columna a snake_case simple.\"\"\"\n    df = df.copy()\n    def _clean(col: str) -> str:\n        col = col.strip().lower()\n        col = re.sub(r\"\\s+\", \"_\", col)\n        col = re.sub(r\"[^a-z0-9_]+\", \"_\", col)\n        col = re.sub(r\"_+\", \"_\", col).strip(\"_\")\n        return col\n    df.columns = [_clean(c) for c in df.columns]\n    return df\n\n\ndef strip_strings(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Trim de espacios en strings (incluye valores tipo '  P1001 ').\"\"\"\n    df = df.copy()\n    for c in df.columns:\n        if df[c].dtype == object:\n            df[c] = df[c].astype(str).str.strip()\n            df[c] = df[c].replace({\"\": np.nan, \"None\": np.nan, \"nan\": np.nan, \"NaN\": np.nan})\n    return df\n\n\n_NUM_CLEAN_RE = re.compile(r\"[^0-9,\\.\\-]+\")  # deja d√≠gitos, coma, punto y signo\n\n\ndef parse_numeric_series(s: pd.Series) -> pd.Series:\n    \"\"\"Convierte series con n√∫meros 'sucios' (‚Ç¨, comas, miles) a float.\n\n    Heur√≠stica:\n    - Elimina s√≠mbolos (‚Ç¨, EUR, espacios, etc.)\n    - Si hay coma y punto: asume que la coma es separador de miles y el punto decimal (ej: 1,234.56)\n    - Si solo hay coma: asume coma decimal (ej: 123,45)\n    \"\"\"\n    if s is None:\n        return s\n    s0 = s.astype(str)\n\n    # Mant√©n NaNs\n    s0 = s0.replace({\"None\": np.nan, \"nan\": np.nan, \"NaN\": np.nan})\n\n    # Limpia s√≠mbolos\n    s1 = s0.str.replace(_NUM_CLEAN_RE, \"\", regex=True)\n\n    def _to_float(x: str):\n        if x is None or (isinstance(x, float) and np.isnan(x)):\n            return np.nan\n        x = str(x)\n        if x.strip() == \"\":\n            return np.nan\n\n        # Casos con ambos separadores\n        if \",\" in x and \".\" in x:\n            # ejemplo 1,234.56 => quitar comas\n            x = x.replace(\",\", \"\")\n        elif \",\" in x and \".\" not in x:\n            # ejemplo 123,45 => coma decimal\n            x = x.replace(\",\", \".\")\n        # else: solo punto o solo d√≠gitos\n\n        try:\n            return float(x)\n        except Exception:\n            return np.nan\n\n    return s1.map(_to_float)\n\n\n_TRUE_SET = {\"1\", \"true\", \"t\", \"yes\", \"y\", \"si\", \"s√≠\", \"s\"}\n_FALSE_SET = {\"0\", \"false\", \"f\", \"no\", \"n\"}\n\ndef parse_bool_series(s: pd.Series) -> pd.Series:\n    \"\"\"Parsea booleanos heterog√©neos a 0/1 (float para permitir NaN).\"\"\"\n    s0 = s.astype(str).str.strip().str.lower()\n    s0 = s0.replace({\"\": np.nan, \"none\": np.nan, \"nan\": np.nan})\n    def _map(x):\n        if x is None or (isinstance(x, float) and np.isnan(x)):\n            return np.nan\n        if x in _TRUE_SET:\n            return 1.0\n        if x in _FALSE_SET:\n            return 0.0\n        return np.nan\n    return s0.map(_map)\n\n\ndef guess_column_roles(df: pd.DataFrame, target_col: str | None = None) -> dict:\n    \"\"\"Devuelve una propuesta de roles: ids / categoricas / numericas / fechas.\"\"\"\n    cols = list(df.columns)\n    roles = {\"id\": [], \"categorical\": [], \"numeric\": [], \"datetime\": []}\n\n    # Heur√≠sticas b√°sicas\n    for c in cols:\n        if target_col and c == target_col:\n            continue\n        if c.endswith(\"_id\") or c in {\"customer_id\", \"product_id\", \"ticket_id\", \"order_id\"}:\n            roles[\"id\"].append(c)\n        elif \"date\" in c or \"fecha\" in c or c.endswith(\"_dt\") or c.endswith(\"_datetime\") or c.endswith(\"_ts\"):\n            roles[\"datetime\"].append(c)\n        else:\n            roles[\"categorical\"].append(c)\n\n    return roles\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2) Carga de datasets de features\n\nIntentamos localizar los ficheros en `output_features/`.  \nSi tus notebooks anteriores exportaron en otra carpeta, ajusta `FEATURES_DIR`.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# ============================================\n# 2) Carga de datasets (features)\n# ============================================\nFEATURES_DIR = Path(\"output_features\")\nOUTPUT_DIR = Path(\"output_ml\")\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\npaths = {\n    \"clientes\": FEATURES_DIR / \"features_clientes.csv\",\n    \"productos\": FEATURES_DIR / \"features_productos.csv\",\n    \"propension\": FEATURES_DIR / \"dataset_propension_snapshots.csv\",\n}\n\nloaded = {}\nfor k, p in paths.items():\n    if p.exists():\n        df = load_csv_robust(p)\n        df = standardize_column_names(strip_strings(df))\n        loaded[k] = df\n        print(f\"‚úÖ Cargado: {k} -> {p} | shape={df.shape}\")\n    else:\n        print(f\"‚ö†Ô∏è No encontrado: {k} -> {p}\")\n\nloaded.keys()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3) Perfilado r√°pido (formativo)\n\nAntes de preprocesar, hacemos un chequeo r√°pido:\n- tama√±os, nulos, duplicados exactos\n- columnas con cardinalidad alta (posibles IDs)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# ============================================\n# 3) Perfilado r√°pido\n# ============================================\n\ndef quick_profile(df: pd.DataFrame, name: str, max_unique_show: int = 10) -> pd.DataFrame:\n    rows = []\n    n = len(df)\n    dup_exact = int(df.duplicated().sum())\n    for c in df.columns:\n        nulls = int(df[c].isna().sum())\n        nunique = int(df[c].nunique(dropna=True))\n        sample = df[c].dropna().astype(str).head(max_unique_show).tolist()\n        rows.append({\n            \"dataset\": name,\n            \"column\": c,\n            \"dtype\": str(df[c].dtype),\n            \"rows\": n,\n            \"nulls\": nulls,\n            \"null_pct\": round(100 * nulls / n, 2) if n else np.nan,\n            \"nunique\": nunique,\n            \"sample_values\": sample\n        })\n    prof = pd.DataFrame(rows).sort_values([\"null_pct\", \"nunique\"], ascending=[False, False])\n    print(f\"\\nüìå {name}: shape={df.shape} | duplicated_rows_exact={dup_exact}\")\n    display(prof.head(20))\n    return prof\n\nprofiles = {}\nfor name, df in loaded.items():\n    profiles[name] = quick_profile(df, name)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4) Construcci√≥n de un preprocesador ML (escala + one-hot)\n\nVamos a crear funciones reutilizables para:\n- identificar columnas num√©ricas/categ√≥ricas\n- aplicar **imputaci√≥n** de nulos\n- aplicar **escalado** a num√©ricas (StandardScaler o MinMaxScaler)\n- aplicar **one-hot** a categ√≥ricas (OneHotEncoder)\n\nüí° Nota did√°ctica: guardamos tambi√©n la lista de *feature names* resultante.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# ============================================\n# 4) Preprocesamiento (sklearn): build + transform + export\n# ============================================\n\ndef detect_numeric_columns(df: pd.DataFrame, candidate_cols: list[str]) -> list[str]:\n    \"\"\"Intenta detectar columnas num√©ricas dentro de un conjunto de candidatas.\"\"\"\n    numeric_cols = []\n    for c in candidate_cols:\n        # intencionalmente tolerante: si al parsear hay suficientes valores num√©ricos, consideramos num√©rica\n        parsed = parse_numeric_series(df[c])\n        ok_ratio = parsed.notna().mean() if len(parsed) else 0\n        # umbral did√°ctico: si m√°s del 60% parsea, la tratamos como num√©rica\n        if ok_ratio >= 0.60:\n            numeric_cols.append(c)\n    return numeric_cols\n\n\ndef split_numeric_categorical(df: pd.DataFrame, target_col: str | None, id_cols: list[str]) -> tuple[list[str], list[str]]:\n    cols = [c for c in df.columns if c != target_col]\n    cols = [c for c in cols if c not in id_cols]\n    # intento: detectar num√©ricas\n    numeric_cols = detect_numeric_columns(df, cols)\n    cat_cols = [c for c in cols if c not in numeric_cols]\n    return numeric_cols, cat_cols\n\n\ndef build_preprocessor(numeric_cols: list[str], categorical_cols: list[str], scaler: str = \"standard\") -> ColumnTransformer:\n    # escalador\n    if scaler == \"minmax\":\n        num_scaler = MinMaxScaler()\n    else:\n        num_scaler = StandardScaler()\n\n    # OneHotEncoder: compatibilidad entre versiones (sparse vs sparse_output)\n    try:\n        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n    except TypeError:\n        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n\n    numeric_pipe = Pipeline(steps=[\n        (\"imputer\", SimpleImputer(strategy=\"median\")),\n        (\"scaler\", num_scaler),\n    ])\n\n    categorical_pipe = Pipeline(steps=[\n        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n        (\"onehot\", ohe),\n    ])\n\n    preprocessor = ColumnTransformer(\n        transformers=[\n            (\"num\", numeric_pipe, numeric_cols),\n            (\"cat\", categorical_pipe, categorical_cols),\n        ],\n        remainder=\"drop\"\n    )\n    return preprocessor\n\n\ndef get_feature_names(preprocessor: ColumnTransformer) -> list[str]:\n    \"\"\"Recupera nombres de features tras ColumnTransformer.\"\"\"\n    feature_names = []\n\n    # num\n    try:\n        num_features = list(preprocessor.transformers_[0][2])\n    except Exception:\n        num_features = []\n    feature_names.extend([f\"num__{c}\" for c in num_features])\n\n    # cat (onehot)\n    cat_cols = list(preprocessor.transformers_[1][2]) if len(preprocessor.transformers_) > 1 else []\n    if cat_cols:\n        ohe = preprocessor.named_transformers_[\"cat\"].named_steps[\"onehot\"]\n        try:\n            cat_names = ohe.get_feature_names_out(cat_cols).tolist()\n        except Exception:\n            # fallback simple\n            cat_names = []\n            for i, c in enumerate(cat_cols):\n                cat_names.append(f\"{c}_*\")\n        feature_names.extend([f\"cat__{n}\" for n in cat_names])\n\n    return feature_names\n\n\ndef coerce_numeric_columns(df: pd.DataFrame, numeric_cols: list[str]) -> pd.DataFrame:\n    \"\"\"Convierte columnas num√©ricas detectadas a float de forma tolerante.\"\"\"\n    df = df.copy()\n    for c in numeric_cols:\n        df[c] = parse_numeric_series(df[c])\n    return df\n\n\ndef preprocess_dataframe_for_ml(\n    df_raw: pd.DataFrame,\n    target_col: str | None,\n    id_cols: list[str],\n    scaler: str = \"standard\",\n    test_size: float = 0.2,\n    random_state: int = 42,\n) -> dict:\n    \"\"\"Devuelve X_train/X_test/y_train/y_test + transformer + feature_names (si hay target).\"\"\"\n    df = df_raw.copy()\n\n    # Detectar columnas num/cat\n    numeric_cols, categorical_cols = split_numeric_categorical(df, target_col=target_col, id_cols=id_cols)\n    print(f\"Numeric cols ({len(numeric_cols)}): {numeric_cols[:10]}{'...' if len(numeric_cols)>10 else ''}\")\n    print(f\"Categorical cols ({len(categorical_cols)}): {categorical_cols[:10]}{'...' if len(categorical_cols)>10 else ''}\")\n\n    # Coerce num√©ricas (para que el imputador/escalador funcionen)\n    df = coerce_numeric_columns(df, numeric_cols)\n\n    # Target\n    y = None\n    if target_col and target_col in df.columns:\n        # target puede venir como texto -> intenta num√©rico; si no, booleans\n        y_num = parse_numeric_series(df[target_col])\n        if y_num.notna().mean() >= 0.8:\n            y = y_num.astype(float)\n        else:\n            y = parse_bool_series(df[target_col])\n        # fallback final: si no parsea bien, deja como texto\n        if y is None or y.isna().all():\n            y = df[target_col]\n\n    # X (sin id_cols, sin target)\n    drop_cols = set(id_cols)\n    if target_col:\n        drop_cols.add(target_col)\n    X = df.drop(columns=[c for c in drop_cols if c in df.columns], errors=\"ignore\")\n\n    preprocessor = build_preprocessor(numeric_cols=[c for c in numeric_cols if c in X.columns],\n                                      categorical_cols=[c for c in categorical_cols if c in X.columns],\n                                      scaler=scaler)\n\n    X_trans = preprocessor.fit_transform(X)\n    feature_names = get_feature_names(preprocessor)\n\n    # Empaquetar a DataFrame\n    X_trans_df = pd.DataFrame(X_trans, columns=feature_names)\n\n    # Split si hay y num√©rico razonable\n    if y is not None:\n        # elimina filas con y NaN (did√°ctico: para ML suele ser obligatorio)\n        mask = ~pd.isna(y)\n        X_trans_df2 = X_trans_df.loc[mask].reset_index(drop=True)\n        y2 = y.loc[mask].reset_index(drop=True)\n\n        X_train, X_test, y_train, y_test = train_test_split(\n            X_trans_df2, y2, test_size=test_size, random_state=random_state, stratify=None\n        )\n        return {\n            \"X_all\": X_trans_df2,\n            \"y_all\": y2,\n            \"X_train\": X_train,\n            \"X_test\": X_test,\n            \"y_train\": y_train,\n            \"y_test\": y_test,\n            \"preprocessor\": preprocessor,\n            \"feature_names\": feature_names,\n            \"numeric_cols\": numeric_cols,\n            \"categorical_cols\": categorical_cols,\n        }\n\n    return {\n        \"X_all\": X_trans_df,\n        \"preprocessor\": preprocessor,\n        \"feature_names\": feature_names,\n        \"numeric_cols\": numeric_cols,\n        \"categorical_cols\": categorical_cols,\n    }\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5) Dataset ML-ready de Clientes (churn/RFM)\n\nEn el Notebook 5 se generaba una etiqueta did√°ctica t√≠pica:\n- `label_churn_180d` (1 si no compra en los √∫ltimos 180 d√≠as, 0 en caso contrario)\n\nEn este notebook:\n- escalamos num√©ricas + one-hot en categ√≥ricas\n- exportamos un dataset **100% num√©rico** listo para ML/BI\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# ============================================\n# 5) Clientes: ML-ready (con label de churn si existe)\n# ============================================\nif \"clientes\" not in loaded:\n    raise RuntimeError(\"No se encontr√≥ features_clientes.csv en output_features/. Ejecuta el Notebook 5 primero.\")\n\ndf_cli = loaded[\"clientes\"].copy()\n\n# Columnas t√≠picas que solemos tratar como IDs (ajusta si tu dataset difiere)\nid_cols_cli = [c for c in df_cli.columns if c in {\"customer_id\", \"cliente_id\", \"id_cliente\"}]\ntarget_cli = \"label_churn_180d\" if \"label_churn_180d\" in df_cli.columns else None\n\nprint(\"Target:\", target_cli)\nprint(\"ID cols:\", id_cols_cli)\n\nprep_cli = preprocess_dataframe_for_ml(\n    df_raw=df_cli,\n    target_col=target_cli,\n    id_cols=id_cols_cli,\n    scaler=\"standard\",  # cambia a 'minmax' si quieres 0..1\n)\n\nX_cli = prep_cli[\"X_all\"]\ny_cli = prep_cli.get(\"y_all\", None)\n\ndisplay(X_cli.head())\nif y_cli is not None:\n    display(y_cli.value_counts(dropna=False).head(10))\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Export clientes ML-ready\nclients_ml_path = OUTPUT_DIR / \"FraSoHome_clientes_ML_ready.csv\"\nX_cli_export = X_cli.copy()\n\n# si existe target, lo a√±adimos\nif y_cli is not None:\n    X_cli_export[target_cli] = y_cli.values\n\n# (opcional) a√±ade id si existe para trazabilidad BI\nif id_cols_cli:\n    # mant√©n una copia de IDs \"raw\"\n    X_cli_export.insert(0, id_cols_cli[0], df_cli.loc[~pd.isna(y_cli) if y_cli is not None else df_cli.index, id_cols_cli[0]].values)\n\nX_cli_export.to_csv(clients_ml_path, index=False, encoding=\"utf-8\")\nprint(f\"‚úÖ Exportado: {clients_ml_path} | shape={X_cli_export.shape}\")\n\n# Metadata (diccionario de variables)\nmeta_cli = {\n    \"dataset\": \"clientes\",\n    \"target\": target_cli,\n    \"id_cols\": id_cols_cli,\n    \"numeric_cols_detected\": prep_cli[\"numeric_cols\"],\n    \"categorical_cols_detected\": prep_cli[\"categorical_cols\"],\n    \"n_features_after_encoding\": len(prep_cli[\"feature_names\"]),\n    \"feature_names\": prep_cli[\"feature_names\"][:200],  # l√≠mite did√°ctico; ajusta si quieres todas\n    \"scaler\": \"standard\",\n}\nmeta_path = OUTPUT_DIR / \"FraSoHome_clientes_ML_ready_metadata.json\"\nmeta_path.write_text(json.dumps(meta_cli, ensure_ascii=False, indent=2), encoding=\"utf-8\")\nprint(f\"‚úÖ Metadata: {meta_path}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6) Dataset ML-ready de Propensi√≥n (si existe)\n\nSi has generado `dataset_propension_snapshots.csv` en Notebook 5, aqu√≠ lo preprocesamos tambi√©n.\nSuele incluir un target tipo `label_buy_horizon` (nombre puede variar).  \nEste bloque detecta autom√°ticamente una columna target si existe.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# ============================================\n# 6) Propensi√≥n (si existe)\n# ============================================\nif \"propension\" in loaded:\n    df_prop = loaded[\"propension\"].copy()\n\n    # Heur√≠stica: buscar targets comunes\n    possible_targets = [c for c in df_prop.columns if c.startswith(\"label_\") or c in {\"target\", \"y\", \"will_buy\"}]\n    target_prop = possible_targets[0] if possible_targets else None\n\n    id_cols_prop = [c for c in df_prop.columns if c.endswith(\"_id\") or c in {\"customer_id\", \"product_id\", \"snapshot_date\"}]\n    print(\"Target propensi√≥n detectado:\", target_prop)\n    print(\"ID cols (propensi√≥n):\", id_cols_prop)\n\n    prep_prop = preprocess_dataframe_for_ml(\n        df_raw=df_prop,\n        target_col=target_prop,\n        id_cols=id_cols_prop,\n        scaler=\"minmax\"  # en propensi√≥n suele ser c√≥modo 0..1\n    )\n\n    X_prop = prep_prop[\"X_all\"]\n    y_prop = prep_prop.get(\"y_all\", None)\n\n    prop_path = OUTPUT_DIR / \"FraSoHome_propension_ML_ready.csv\"\n    X_prop_export = X_prop.copy()\n    if y_prop is not None and target_prop:\n        X_prop_export[target_prop] = y_prop.values\n\n    X_prop_export.to_csv(prop_path, index=False, encoding=\"utf-8\")\n    print(f\"‚úÖ Exportado: {prop_path} | shape={X_prop_export.shape}\")\nelse:\n    print(\"‚ÑπÔ∏è No hay dataset de propensi√≥n. (No existe output_features/dataset_propension_snapshots.csv)\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7) Dataset ML-ready de Productos (clustering / regresi√≥n / BI)\n\nPara productos normalmente no hay target. Preparamos el dataset para:\n- clustering (p.ej. KMeans)  \n- regresi√≥n (p.ej. predicci√≥n de devoluciones, margen, etc.)  \n- BI (cuadros de mando con variables normalizadas)\n\nLa idea did√°ctica: estandarizamos num√©ricas + one-hot de categor√≠a/marca/canal, etc.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# ============================================\n# 7) Productos (si existe)\n# ============================================\nif \"productos\" in loaded:\n    df_prod = loaded[\"productos\"].copy()\n    id_cols_prod = [c for c in df_prod.columns if c in {\"product_id\", \"sku\", \"id_producto\"}]\n\n    prep_prod = preprocess_dataframe_for_ml(\n        df_raw=df_prod,\n        target_col=None,\n        id_cols=id_cols_prod,\n        scaler=\"standard\"\n    )\n\n    X_prod = prep_prod[\"X_all\"]\n    prod_path = OUTPUT_DIR / \"FraSoHome_productos_ML_ready.csv\"\n\n    X_prod_export = X_prod.copy()\n    # BI: a√±ade product_id como trazabilidad\n    if id_cols_prod:\n        X_prod_export.insert(0, id_cols_prod[0], df_prod[id_cols_prod[0]].values)\n\n    X_prod_export.to_csv(prod_path, index=False, encoding=\"utf-8\")\n    print(f\"‚úÖ Exportado: {prod_path} | shape={X_prod_export.shape}\")\nelse:\n    print(\"‚ÑπÔ∏è No hay features_productos.csv en output_features/\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8) Comprobaciones finales (calidad ML-ready)\n\n- ¬øQuedan nulos en los datasets exportados?\n- ¬øQu√© tama√±o tienen (n_features)?\n- ¬øQu√© proporci√≥n de columnas son one-hot (alta dimensionalidad)?\n\nEste bloque ayuda a discutir trade-offs (por ejemplo, demasiadas columnas one-hot, etc.).\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# ============================================\n# 8) Checks finales\n# ============================================\n\ndef check_ml_dataset(path: Path, name: str, max_cols_show: int = 30):\n    df = pd.read_csv(path, dtype=str, encoding=\"utf-8\")\n    # intenta medir nulos\n    null_pct = (df.isna().mean() * 100).sort_values(ascending=False)\n    print(f\"\\n‚úÖ {name} -> {path.name} | shape={df.shape}\")\n    print(\"Top columnas con nulos (%):\")\n    display(null_pct.head(10))\n    print(\"Muestra de columnas:\", df.columns[:max_cols_show].tolist())\n\nfor p in OUTPUT_DIR.glob(\"*.csv\"):\n    check_ml_dataset(p, p.stem)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 9) Pr√≥ximos pasos sugeridos (para el curso)\n\n- Entrenar un primer modelo baseline (LogReg / RandomForest) con `FraSoHome_clientes_ML_ready.csv`.\n- Evaluar con AUC/Accuracy y revisar:\n  - impacto de la imputaci√≥n,\n  - sensibilidad a outliers,\n  - necesidad de balanceo,\n  - ingenier√≠a adicional (tendencias temporales, ventanas m√≥viles).\n- Para basket: transformar `basket_long.csv` a matriz one-hot y aplicar Apriori / FP-Growth en un notebook adicional.\n\n---\n\nüí° Si quieres, puedo generarte un **Notebook 7 (opcional)**:  \n`Baseline Modeling` (churn y propensi√≥n) con evaluaci√≥n, *cross-validation* y explicaci√≥n did√°ctica.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
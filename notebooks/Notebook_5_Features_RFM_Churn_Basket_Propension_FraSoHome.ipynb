{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b283a10",
   "metadata": {},
   "source": [
    "# FraSoHome – Notebook 5: Cálculo de Features (Cliente y Producto)\n",
    "\n",
    "Este notebook continúa el caso práctico **FraSoHome** y tiene un objetivo **formativo**: a partir de una **fact table integrada** de transacciones omnicanal (online + tienda + devoluciones), calcularemos **features** para:\n",
    "\n",
    "- Segmentación **RFM** (Recencia, Frecuencia, Monetario)\n",
    "- Casos de uso de **abandono (churn)** (definición de etiqueta y variables)\n",
    "- **Propensión de compra** (dataset por *snapshots*)\n",
    "- **Análisis de cesta** (preparación de datos de transacciones)\n",
    "- KPIs y features a nivel de **producto/categoría/canal**\n",
    "\n",
    "### Enfoque didáctico\n",
    "- Cargamos datos en modo **raw** (`dtype=str`) para preservar errores.\n",
    "- Aplicamos funciones reutilizables que reciben **dataframes como parámetros**.\n",
    "- Generamos salidas en CSV dentro de `output_features/`.\n",
    "\n",
    "> Nota: Los datos contienen **errores intencionales** (IDs huérfanos, formatos mixtos, etc.).  \n",
    "> Aquí nos centramos en **feature engineering** tras una limpieza mínima.  \n",
    "> Si has ejecutado el Notebook 3/4 con datasets ya limpios/integrados, los usaremos si existen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dc67dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Optional, Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 140)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d045c2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# Configuración de rutas\n",
    "# =========================\n",
    "DATA_DIR = \".\"  # carpeta donde están los CSV (ajusta si lo necesitas)\n",
    "OUTPUT_DIR = \"output_features\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "PATHS = {\n",
    "    \"crm\": os.path.join(DATA_DIR, \"crm.csv\"),\n",
    "    \"productos\": os.path.join(DATA_DIR, \"productos.csv\"),\n",
    "    \"tiendas\": os.path.join(DATA_DIR, \"tiendas.csv\"),\n",
    "    \"stock_diario\": os.path.join(DATA_DIR, \"stock_diario.csv\"),\n",
    "    # Preferimos la fact integrada del Notebook 4 si existe\n",
    "    \"fact_integrada\": os.path.join(DATA_DIR, \"output_integrated\", \"fact_transacciones_integrada.csv\"),\n",
    "    # Fallback: fact preconstruida\n",
    "    \"fact_fallback\": os.path.join(DATA_DIR, \"fact_transacciones.csv\"),\n",
    "}\n",
    "\n",
    "for k, p in PATHS.items():\n",
    "    print((\"OK  \" if os.path.exists(p) else \"MISS\") + f\"{k:>18}: {p}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff90dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================================\n",
    "# Utilidades reutilizables (formato, fechas, números, IDs)\n",
    "# ==========================================================\n",
    "\n",
    "def load_csv_raw(path: str, encoding: str = \"utf-8\") -> pd.DataFrame:\n",
    "    \"\"\"Carga un CSV en modo raw (dtype=str) para NO perder errores intencionales.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"No existe el archivo: {path}\")\n",
    "    return pd.read_csv(\n",
    "        path,\n",
    "        dtype=str,\n",
    "        encoding=encoding,\n",
    "        keep_default_na=False,\n",
    "        na_values=[\"\", \"NULL\", \"null\", \"None\", \"NA\", \"N/A\"]\n",
    "    )\n",
    "\n",
    "def standardize_id(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Normaliza IDs (strip, uppercase) y convierte valores tipo GUEST/nan a NaN.\"\"\"\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = s.replace({\"\": np.nan, \"nan\": np.nan, \"None\": np.nan, \"NULL\": np.nan, \"N/A\": np.nan})\n",
    "    s = s.str.replace(r\"\\s+\", \"\", regex=True)   # quita espacios internos\n",
    "    s = s.str.upper()\n",
    "    s = s.replace({\"GUEST\": np.nan, \"INVITADO\": np.nan})\n",
    "    return s\n",
    "\n",
    "_MONTHS_ES = {\n",
    "    \"enero\": 1, \"febrero\": 2, \"marzo\": 3, \"abril\": 4, \"mayo\": 5, \"junio\": 6,\n",
    "    \"julio\": 7, \"agosto\": 8, \"septiembre\": 9, \"setiembre\": 9, \"octubre\": 10,\n",
    "    \"noviembre\": 11, \"diciembre\": 12\n",
    "}\n",
    "\n",
    "def _parse_spanish_text_date(value: str):\n",
    "    \"\"\"Parsea fechas tipo '10 de Enero de 2023' (con o sin hora).\"\"\"\n",
    "    if value is None or (isinstance(value, float) and np.isnan(value)):\n",
    "        return None\n",
    "    txt = str(value).strip().lower()\n",
    "    m = re.search(r\"(\\d{1,2})\\s+de\\s+([a-záéíóúñ]+)\\s+de\\s+(\\d{4})(?:\\s+(\\d{1,2}):(\\d{2}))?\", txt)\n",
    "    if not m:\n",
    "        return None\n",
    "\n",
    "    day = int(m.group(1))\n",
    "    month_name = (m.group(2)\n",
    "                  .replace(\"á\",\"a\").replace(\"é\",\"e\").replace(\"í\",\"i\")\n",
    "                  .replace(\"ó\",\"o\").replace(\"ú\",\"u\"))\n",
    "    month = _MONTHS_ES.get(month_name)\n",
    "    if month is None:\n",
    "        return None\n",
    "\n",
    "    year = int(m.group(3))\n",
    "    hh = int(m.group(4)) if m.group(4) else 0\n",
    "    mm = int(m.group(5)) if m.group(5) else 0\n",
    "\n",
    "    try:\n",
    "        return datetime(year, month, day, hh, mm)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def parse_datetime_es(series: pd.Series, dayfirst: bool = True) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Parsea fechas soportando:\n",
    "    - ISO (YYYY-MM-DD, YYYY-MM-DD HH:MM:SS)\n",
    "    - DD/MM/YYYY o DD/MM/YY\n",
    "    - Texto español: '10 de Enero de 2023'\n",
    "    \"\"\"\n",
    "    s = series.copy()\n",
    "    dt1 = pd.to_datetime(s, errors=\"coerce\", dayfirst=dayfirst, utc=False)\n",
    "\n",
    "    mask = dt1.isna() & s.notna()\n",
    "    if mask.any():\n",
    "        parsed = [_parse_spanish_text_date(v) for v in s[mask].astype(str).tolist()]\n",
    "        dt1.loc[mask] = pd.to_datetime(parsed, errors=\"coerce\")\n",
    "\n",
    "    return dt1\n",
    "\n",
    "def parse_numeric_mixed(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Convierte strings numéricos con formatos mixtos a float:\n",
    "    - '€1.234,56' / '1.234,56' / '1234.56' / '1,234.56'\n",
    "    \"\"\"\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = s.replace({\"\": np.nan, \"nan\": np.nan, \"None\": np.nan, \"NULL\": np.nan, \"N/A\": np.nan})\n",
    "\n",
    "    s = (s.str.replace(\"€\", \"\", regex=False)\n",
    "           .str.replace(\"EUR\", \"\", regex=False)\n",
    "           .str.replace(\"eur\", \"\", regex=False))\n",
    "\n",
    "    # deja solo dígitos, coma, punto y signo\n",
    "    s = s.str.replace(r\"[^\\d,.\\-]\", \"\", regex=True)\n",
    "\n",
    "    def _to_float(x):\n",
    "        if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "            return np.nan\n",
    "        x = str(x)\n",
    "        if x == \"\":\n",
    "            return np.nan\n",
    "\n",
    "        if \",\" in x and \".\" in x:\n",
    "            # decide cuál es el separador decimal por posición\n",
    "            if x.rfind(\",\") > x.rfind(\".\"):\n",
    "                x2 = x.replace(\".\", \"\").replace(\",\", \".\")\n",
    "            else:\n",
    "                x2 = x.replace(\",\", \"\")\n",
    "        elif \",\" in x and \".\" not in x:\n",
    "            x2 = x.replace(\",\", \".\")\n",
    "        else:\n",
    "            x2 = x\n",
    "\n",
    "        try:\n",
    "            return float(x2)\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "\n",
    "    return s.apply(_to_float)\n",
    "\n",
    "def safe_ratio(num: pd.Series, den: pd.Series) -> pd.Series:\n",
    "    \"\"\"Evita divisiones por cero.\"\"\"\n",
    "    den0 = den.fillna(0)\n",
    "    return np.where(den0 == 0, np.nan, num / den0)\n",
    "\n",
    "def describe_quick(df: pd.DataFrame, name: str, n: int = 5) -> None:\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"shape:\", df.shape)\n",
    "    display(df.head(n))\n",
    "    na = df.isna().mean().sort_values(ascending=False)\n",
    "    print(\"\\n% nulos (top 10):\")\n",
    "    display((na.head(10) * 100).round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc67990f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# Carga de datasets\n",
    "# =========================\n",
    "crm = load_csv_raw(PATHS[\"crm\"]) if os.path.exists(PATHS[\"crm\"]) else None\n",
    "productos = load_csv_raw(PATHS[\"productos\"]) if os.path.exists(PATHS[\"productos\"]) else None\n",
    "tiendas = load_csv_raw(PATHS[\"tiendas\"]) if os.path.exists(PATHS[\"tiendas\"]) else None\n",
    "stock = load_csv_raw(PATHS[\"stock_diario\"]) if os.path.exists(PATHS[\"stock_diario\"]) else None\n",
    "\n",
    "if os.path.exists(PATHS[\"fact_integrada\"]):\n",
    "    fact = load_csv_raw(PATHS[\"fact_integrada\"])\n",
    "    FACT_SOURCE = \"fact_integrada\"\n",
    "else:\n",
    "    fact = load_csv_raw(PATHS[\"fact_fallback\"])\n",
    "    FACT_SOURCE = \"fact_fallback\"\n",
    "\n",
    "print(f\"\\nUsando fact: {FACT_SOURCE} -> filas={len(fact):,} cols={fact.shape[1]}\")\n",
    "describe_quick(fact, \"fact (raw)\", n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0c0d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================================\n",
    "# Tipado mínimo para feature engineering (fact + dimensiones)\n",
    "# ==========================================================\n",
    "\n",
    "def ensure_fact_types(fact_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Asegura que la fact tenga:\n",
    "    - customer_id_std / product_id_std / store_id_std estandarizados\n",
    "    - event_dt (datetime)\n",
    "    - amount_signed (float) y quantity_signed (float)\n",
    "    Mantiene columnas originales para trazabilidad.\n",
    "    \"\"\"\n",
    "    df = fact_raw.copy()\n",
    "\n",
    "    # IDs\n",
    "    if \"customer_id_std\" in df.columns:\n",
    "        df[\"customer_id_std\"] = standardize_id(df[\"customer_id_std\"])\n",
    "    elif \"customer_id\" in df.columns:\n",
    "        df[\"customer_id_std\"] = standardize_id(df[\"customer_id\"])\n",
    "    elif \"customer_id_raw\" in df.columns:\n",
    "        df[\"customer_id_std\"] = standardize_id(df[\"customer_id_raw\"])\n",
    "    else:\n",
    "        df[\"customer_id_std\"] = np.nan\n",
    "\n",
    "    if \"product_id_std\" in df.columns:\n",
    "        df[\"product_id_std\"] = standardize_id(df[\"product_id_std\"])\n",
    "    elif \"product_id\" in df.columns:\n",
    "        df[\"product_id_std\"] = standardize_id(df[\"product_id\"])\n",
    "    elif \"product_id_raw\" in df.columns:\n",
    "        df[\"product_id_std\"] = standardize_id(df[\"product_id_raw\"])\n",
    "    else:\n",
    "        df[\"product_id_std\"] = np.nan\n",
    "\n",
    "    if \"store_id_std\" in df.columns:\n",
    "        df[\"store_id_std\"] = standardize_id(df[\"store_id_std\"])\n",
    "    elif \"store_id\" in df.columns:\n",
    "        df[\"store_id_std\"] = standardize_id(df[\"store_id\"])\n",
    "    elif \"store_id_raw\" in df.columns:\n",
    "        df[\"store_id_std\"] = standardize_id(df[\"store_id_raw\"])\n",
    "    else:\n",
    "        df[\"store_id_std\"] = np.nan\n",
    "\n",
    "    # Fecha\n",
    "    if \"fecha_movimiento\" in df.columns:\n",
    "        df[\"event_dt\"] = parse_datetime_es(df[\"fecha_movimiento\"])\n",
    "    elif \"fecha_hora\" in df.columns:\n",
    "        df[\"event_dt\"] = parse_datetime_es(df[\"fecha_hora\"])\n",
    "    elif \"fecha_movimiento_raw\" in df.columns:\n",
    "        df[\"event_dt\"] = parse_datetime_es(df[\"fecha_movimiento_raw\"])\n",
    "    else:\n",
    "        df[\"event_dt\"] = pd.NaT\n",
    "\n",
    "    # Cantidad\n",
    "    if \"cantidad_signed\" in df.columns:\n",
    "        df[\"quantity_signed\"] = parse_numeric_mixed(df[\"cantidad_signed\"])\n",
    "    elif \"cantidad\" in df.columns:\n",
    "        df[\"quantity_signed\"] = parse_numeric_mixed(df[\"cantidad\"])\n",
    "    elif \"cantidad_raw\" in df.columns:\n",
    "        df[\"quantity_signed\"] = parse_numeric_mixed(df[\"cantidad_raw\"])\n",
    "    else:\n",
    "        df[\"quantity_signed\"] = np.nan\n",
    "\n",
    "    # Importe\n",
    "    if \"importe_signed_num\" in df.columns:\n",
    "        df[\"amount_signed\"] = parse_numeric_mixed(df[\"importe_signed_num\"])\n",
    "    elif \"importe_signed\" in df.columns:\n",
    "        df[\"amount_signed\"] = parse_numeric_mixed(df[\"importe_signed\"])\n",
    "    elif \"importe_linea_raw\" in df.columns:\n",
    "        df[\"amount_signed\"] = parse_numeric_mixed(df[\"importe_linea_raw\"])\n",
    "    else:\n",
    "        df[\"amount_signed\"] = np.nan\n",
    "\n",
    "    # Descuento (%)\n",
    "    if \"descuento_pct_num\" in df.columns:\n",
    "        df[\"discount_pct\"] = parse_numeric_mixed(df[\"descuento_pct_num\"])\n",
    "    elif \"descuento_pct_raw\" in df.columns:\n",
    "        df[\"discount_pct\"] = parse_numeric_mixed(df[\"descuento_pct_raw\"])\n",
    "    else:\n",
    "        df[\"discount_pct\"] = np.nan\n",
    "\n",
    "    # Canal\n",
    "    if \"canal_origen\" not in df.columns:\n",
    "        df[\"canal_origen\"] = df[\"canal\"] if \"canal\" in df.columns else np.nan\n",
    "\n",
    "    # Tipo movimiento\n",
    "    if \"tipo_movimiento\" not in df.columns:\n",
    "        if \"es_devolucion\" in df.columns:\n",
    "            df[\"tipo_movimiento\"] = np.where(df[\"es_devolucion\"].astype(str).str.lower().isin([\"1\",\"true\",\"yes\",\"si\",\"sí\"]), \"DEVOLUCION\", \"VENTA\")\n",
    "        else:\n",
    "            df[\"tipo_movimiento\"] = np.nan\n",
    "\n",
    "    # Documento (pedido/ticket)\n",
    "    if \"doc_id_std\" not in df.columns:\n",
    "        if \"doc_id\" in df.columns:\n",
    "            df[\"doc_id_std\"] = standardize_id(df[\"doc_id\"])\n",
    "        elif \"ticket_id\" in df.columns:\n",
    "            df[\"doc_id_std\"] = standardize_id(df[\"ticket_id\"])\n",
    "        elif \"order_id\" in df.columns:\n",
    "            df[\"doc_id_std\"] = standardize_id(df[\"order_id\"])\n",
    "        else:\n",
    "            df[\"doc_id_std\"] = np.nan\n",
    "\n",
    "    df[\"is_return\"] = df[\"tipo_movimiento\"].astype(str).str.upper().str.contains(\"DEV\")\n",
    "    df[\"is_sale\"] = ~df[\"is_return\"]\n",
    "    df[\"event_dt\"] = pd.to_datetime(df[\"event_dt\"], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "fact_t = ensure_fact_types(fact)\n",
    "\n",
    "describe_quick(\n",
    "    fact_t[[\"customer_id_std\",\"product_id_std\",\"store_id_std\",\"event_dt\",\"quantity_signed\",\"amount_signed\",\"discount_pct\",\"canal_origen\",\"tipo_movimiento\",\"doc_id_std\"]].copy(),\n",
    "    \"fact (tipada - columnas clave)\",\n",
    "    n=5\n",
    ")\n",
    "\n",
    "print(\"\\nRango fechas (event_dt):\", fact_t[\"event_dt\"].min(), \"->\", fact_t[\"event_dt\"].max())\n",
    "print(\"Filas con fecha no parseada:\", fact_t[\"event_dt\"].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c096e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================================\n",
    "# Features de cliente (RFM, devoluciones, canal, descuentos)\n",
    "# ==========================================================\n",
    "\n",
    "def compute_customer_features(fact_df: pd.DataFrame, as_of: Optional[pd.Timestamp] = None) -> pd.DataFrame:\n",
    "    df = fact_df.copy()\n",
    "    df = df[df[\"customer_id_std\"].notna()].copy()\n",
    "\n",
    "    sales = df[df[\"is_sale\"]].copy()\n",
    "    returns = df[df[\"is_return\"]].copy()\n",
    "\n",
    "    if as_of is None:\n",
    "        as_of = sales[\"event_dt\"].max()\n",
    "    as_of = pd.to_datetime(as_of)\n",
    "\n",
    "    first_last = sales.groupby(\"customer_id_std\")[\"event_dt\"].agg(first_purchase_dt=\"min\", last_purchase_dt=\"max\").reset_index()\n",
    "    first_last[\"tenure_days\"] = (as_of - first_last[\"first_purchase_dt\"]).dt.days\n",
    "    first_last[\"recency_days\"] = (as_of - first_last[\"last_purchase_dt\"]).dt.days\n",
    "\n",
    "    freq = sales.groupby(\"customer_id_std\")[\"doc_id_std\"].nunique().reset_index(name=\"frequency_docs\")\n",
    "\n",
    "    mon_gross = sales.groupby(\"customer_id_std\")[\"amount_signed\"].sum(min_count=1).reset_index(name=\"monetary_gross\")\n",
    "    mon_net = df.groupby(\"customer_id_std\")[\"amount_signed\"].sum(min_count=1).reset_index(name=\"monetary_net\")\n",
    "\n",
    "    qty_gross = sales.groupby(\"customer_id_std\")[\"quantity_signed\"].sum(min_count=1).reset_index(name=\"units_sold\")\n",
    "    qty_net = df.groupby(\"customer_id_std\")[\"quantity_signed\"].sum(min_count=1).reset_index(name=\"units_net\")\n",
    "\n",
    "    ret_docs = returns.groupby(\"customer_id_std\")[\"doc_id_std\"].nunique().reset_index(name=\"return_docs\")\n",
    "    ret_units = returns.groupby(\"customer_id_std\")[\"quantity_signed\"].sum(min_count=1).abs().reset_index(name=\"units_returned\")\n",
    "    ret_amount = returns.groupby(\"customer_id_std\")[\"amount_signed\"].sum(min_count=1).abs().reset_index(name=\"amount_returned\")\n",
    "\n",
    "    # canal split (ventas)\n",
    "    ch = sales.pivot_table(index=\"customer_id_std\", columns=\"canal_origen\", values=\"amount_signed\", aggfunc=\"sum\", fill_value=0)\n",
    "    ch.columns = [f\"amt_{c.lower()}\" for c in ch.columns]\n",
    "    ch = ch.reset_index()\n",
    "\n",
    "    sales[\"has_discount\"] = sales[\"discount_pct\"].fillna(0) > 0\n",
    "    disc = sales.groupby(\"customer_id_std\").agg(\n",
    "        pct_lines_discount=(\"has_discount\", \"mean\"),\n",
    "        avg_discount_pct=(\"discount_pct\", \"mean\"),\n",
    "    ).reset_index()\n",
    "    disc[\"pct_lines_discount\"] = (disc[\"pct_lines_discount\"] * 100).round(2)\n",
    "\n",
    "    last_channel = (sales.sort_values(\"event_dt\")\n",
    "                         .groupby(\"customer_id_std\")\n",
    "                         .tail(1)[[\"customer_id_std\",\"canal_origen\"]]\n",
    "                         .rename(columns={\"canal_origen\":\"last_channel\"}))\n",
    "\n",
    "    out = (first_last.merge(freq, on=\"customer_id_std\", how=\"left\")\n",
    "                    .merge(mon_gross, on=\"customer_id_std\", how=\"left\")\n",
    "                    .merge(mon_net, on=\"customer_id_std\", how=\"left\")\n",
    "                    .merge(qty_gross, on=\"customer_id_std\", how=\"left\")\n",
    "                    .merge(qty_net, on=\"customer_id_std\", how=\"left\")\n",
    "                    .merge(ret_docs, on=\"customer_id_std\", how=\"left\")\n",
    "                    .merge(ret_units, on=\"customer_id_std\", how=\"left\")\n",
    "                    .merge(ret_amount, on=\"customer_id_std\", how=\"left\")\n",
    "                    .merge(ch, on=\"customer_id_std\", how=\"left\")\n",
    "                    .merge(disc, on=\"customer_id_std\", how=\"left\")\n",
    "                    .merge(last_channel, on=\"customer_id_std\", how=\"left\"))\n",
    "\n",
    "    for c in [\"return_docs\",\"units_returned\",\"amount_returned\"]:\n",
    "        out[c] = out[c].fillna(0)\n",
    "\n",
    "    out[\"return_rate_units\"] = safe_ratio(out[\"units_returned\"], out[\"units_sold\"])\n",
    "    out[\"return_rate_amount\"] = safe_ratio(out[\"amount_returned\"], out[\"monetary_gross\"])\n",
    "    out[\"avg_ticket_gross\"] = safe_ratio(out[\"monetary_gross\"], out[\"frequency_docs\"])\n",
    "\n",
    "    # canal favorito por importe\n",
    "    amt_cols = [c for c in out.columns if c.startswith(\"amt_\")]\n",
    "    def _fav_channel(row):\n",
    "        if not amt_cols:\n",
    "            return np.nan\n",
    "        m = row[amt_cols].max()\n",
    "        if pd.isna(m) or m == 0:\n",
    "            return np.nan\n",
    "        for c in amt_cols:\n",
    "            if row[c] == m:\n",
    "                return c.replace(\"amt_\", \"\")\n",
    "        return np.nan\n",
    "    out[\"fav_channel\"] = out.apply(_fav_channel, axis=1)\n",
    "\n",
    "    # etiqueta didáctica churn: no compra en últimos 180 días\n",
    "    out[\"label_churn_180d\"] = (out[\"recency_days\"] > 180).astype(int)\n",
    "\n",
    "    return out\n",
    "\n",
    "customer_features = compute_customer_features(fact_t)\n",
    "describe_quick(customer_features, \"customer_features (base)\", n=10)\n",
    "customer_features.sort_values(\"monetary_gross\", ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3090d3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================================\n",
    "# RFM Scoring (didáctico)\n",
    "# ==========================================================\n",
    "\n",
    "def rfm_score(df: pd.DataFrame,\n",
    "              recency_col: str = \"recency_days\",\n",
    "              frequency_col: str = \"frequency_docs\",\n",
    "              monetary_col: str = \"monetary_gross\",\n",
    "              q: int = 5) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "\n",
    "    out[\"R_score\"] = pd.qcut(out[recency_col].rank(method=\"first\"), q, labels=list(range(q,0,-1))).astype(int)\n",
    "    out[\"F_score\"] = pd.qcut(out[frequency_col].rank(method=\"first\"), q, labels=list(range(1,q+1))).astype(int)\n",
    "    out[\"M_score\"] = pd.qcut(out[monetary_col].rank(method=\"first\"), q, labels=list(range(1,q+1))).astype(int)\n",
    "\n",
    "    out[\"RFM_score\"] = out[\"R_score\"].astype(str) + out[\"F_score\"].astype(str) + out[\"M_score\"].astype(str)\n",
    "    out[\"RFM_sum\"] = out[[\"R_score\",\"F_score\",\"M_score\"]].sum(axis=1)\n",
    "    return out\n",
    "\n",
    "customer_features_rfm = rfm_score(customer_features)\n",
    "display(customer_features_rfm[[\"customer_id_std\",\"recency_days\",\"frequency_docs\",\"monetary_gross\",\"R_score\",\"F_score\",\"M_score\",\"RFM_score\",\"RFM_sum\"]].head(10))\n",
    "\n",
    "print(\"\\nDistribución RFM_sum:\")\n",
    "display(customer_features_rfm[\"RFM_sum\"].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b5f218",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================================\n",
    "# Enriquecimiento con CRM (tier, puntos, estado, etc.)\n",
    "# ==========================================================\n",
    "\n",
    "def prepare_crm_dim(crm_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = crm_raw.copy()\n",
    "\n",
    "    df[\"customer_id_std\"] = standardize_id(df[\"customer_id\"]) if \"customer_id\" in df.columns else np.nan\n",
    "\n",
    "    if \"puntos_acumulados\" in df.columns:\n",
    "        df[\"puntos_num\"] = parse_numeric_mixed(df[\"puntos_acumulados\"])\n",
    "    else:\n",
    "        df[\"puntos_num\"] = np.nan\n",
    "\n",
    "    if \"fecha_alta_programa\" in df.columns:\n",
    "        df[\"fecha_alta_dt\"] = parse_datetime_es(df[\"fecha_alta_programa\"])\n",
    "    else:\n",
    "        df[\"fecha_alta_dt\"] = pd.NaT\n",
    "\n",
    "    if \"tier_fidelizacion\" in df.columns:\n",
    "        df[\"tier_fidelizacion\"] = df[\"tier_fidelizacion\"].astype(str).str.strip()\n",
    "    else:\n",
    "        df[\"tier_fidelizacion\"] = np.nan\n",
    "\n",
    "    # Deduplicación por customer_id_std\n",
    "    if \"ultima_actualizacion\" in df.columns:\n",
    "        df[\"ultima_actualizacion_dt\"] = parse_datetime_es(df[\"ultima_actualizacion\"])\n",
    "        df = df.sort_values(\"ultima_actualizacion_dt\").drop_duplicates(\"customer_id_std\", keep=\"last\")\n",
    "    else:\n",
    "        df = df.drop_duplicates(\"customer_id_std\", keep=\"last\")\n",
    "\n",
    "    keep = [\"customer_id_std\",\"tier_fidelizacion\",\"puntos_num\",\"estado_cliente\",\"consentimiento_marketing\",\"fecha_alta_dt\",\"ciudad\",\"provincia\",\"codigo_postal\",\"pais\"]\n",
    "    keep = [c for c in keep if c in df.columns]\n",
    "    return df[keep].copy()\n",
    "\n",
    "crm_dim = prepare_crm_dim(crm) if crm is not None else None\n",
    "describe_quick(crm_dim, \"crm_dim (preparada)\", n=5)\n",
    "\n",
    "customer_features_final = customer_features_rfm.merge(crm_dim, on=\"customer_id_std\", how=\"left\") if crm_dim is not None else customer_features_rfm.copy()\n",
    "describe_quick(customer_features_final, \"customer_features_final (con CRM)\", n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53aefc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================================\n",
    "# Features de producto / canal\n",
    "# ==========================================================\n",
    "\n",
    "def compute_product_features(fact_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = fact_df.copy()\n",
    "    df = df[df[\"product_id_std\"].notna()].copy()\n",
    "\n",
    "    sales = df[df[\"is_sale\"]].copy()\n",
    "    returns = df[df[\"is_return\"]].copy()\n",
    "\n",
    "    agg_sales = sales.groupby(\"product_id_std\").agg(\n",
    "        units_sold=(\"quantity_signed\", \"sum\"),\n",
    "        revenue_gross=(\"amount_signed\", \"sum\"),\n",
    "        avg_discount_pct=(\"discount_pct\", \"mean\"),\n",
    "        lines_sold=(\"product_id_std\", \"size\"),\n",
    "        docs_sold=(\"doc_id_std\", \"nunique\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    agg_net = df.groupby(\"product_id_std\").agg(\n",
    "        units_net=(\"quantity_signed\", \"sum\"),\n",
    "        revenue_net=(\"amount_signed\", \"sum\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    agg_returns = returns.groupby(\"product_id_std\").agg(\n",
    "        units_returned=(\"quantity_signed\", lambda s: s.abs().sum()),\n",
    "        amount_returned=(\"amount_signed\", lambda s: s.abs().sum()),\n",
    "        return_docs=(\"doc_id_std\", \"nunique\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    out = agg_sales.merge(agg_net, on=\"product_id_std\", how=\"left\").merge(agg_returns, on=\"product_id_std\", how=\"left\")\n",
    "    out[[\"units_returned\",\"amount_returned\",\"return_docs\"]] = out[[\"units_returned\",\"amount_returned\",\"return_docs\"]].fillna(0)\n",
    "\n",
    "    out[\"return_rate_units\"] = safe_ratio(out[\"units_returned\"], out[\"units_sold\"])\n",
    "    out[\"return_rate_amount\"] = safe_ratio(out[\"amount_returned\"], out[\"revenue_gross\"])\n",
    "\n",
    "    # canal split (ventas)\n",
    "    ch = sales.pivot_table(index=\"product_id_std\", columns=\"canal_origen\", values=\"amount_signed\", aggfunc=\"sum\", fill_value=0)\n",
    "    ch.columns = [f\"amt_{c.lower()}\" for c in ch.columns]\n",
    "    ch = ch.reset_index()\n",
    "\n",
    "    out = out.merge(ch, on=\"product_id_std\", how=\"left\")\n",
    "    return out\n",
    "\n",
    "product_features = compute_product_features(fact_t)\n",
    "describe_quick(product_features, \"product_features (base)\", n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ca0f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================================\n",
    "# Enriquecer productos con maestro + métricas de stock\n",
    "# ==========================================================\n",
    "\n",
    "def prepare_product_dim(prod_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = prod_raw.copy()\n",
    "    df[\"product_id_std\"] = standardize_id(df[\"product_id\"]) if \"product_id\" in df.columns else np.nan\n",
    "\n",
    "    if \"precio_venta\" in df.columns:\n",
    "        df[\"precio_venta_num\"] = parse_numeric_mixed(df[\"precio_venta\"])\n",
    "    if \"coste_unitario\" in df.columns:\n",
    "        df[\"coste_unitario_num\"] = parse_numeric_mixed(df[\"coste_unitario\"])\n",
    "\n",
    "    for c in [\"categoria\",\"subcategoria\",\"marca\",\"estado_producto\",\"nombre_producto\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(str).str.strip()\n",
    "\n",
    "    df = df.drop_duplicates(\"product_id_std\", keep=\"last\")\n",
    "    keep = [\"product_id_std\",\"nombre_producto\",\"categoria\",\"subcategoria\",\"marca\",\"proveedor\",\"material\",\"color\",\n",
    "            \"precio_venta_num\",\"coste_unitario_num\",\"estado_producto\"]\n",
    "    keep = [c for c in keep if c in df.columns]\n",
    "    return df[keep].copy()\n",
    "\n",
    "prod_dim = prepare_product_dim(productos) if productos is not None else None\n",
    "describe_quick(prod_dim, \"prod_dim (preparada)\", n=5)\n",
    "\n",
    "def compute_stock_features(stock_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = stock_raw.copy()\n",
    "    df[\"product_id_std\"] = standardize_id(df[\"product_id\"]) if \"product_id\" in df.columns else np.nan\n",
    "    df[\"store_id_std\"] = standardize_id(df[\"store_id\"]) if \"store_id\" in df.columns else np.nan\n",
    "    df[\"date_dt\"] = parse_datetime_es(df[\"fecha\"]) if \"fecha\" in df.columns else pd.NaT\n",
    "    df[\"stock_cierre_num\"] = parse_numeric_mixed(df[\"stock_cierre\"]) if \"stock_cierre\" in df.columns else np.nan\n",
    "\n",
    "    g = df.groupby(\"product_id_std\").agg(\n",
    "        avg_stock=(\"stock_cierre_num\",\"mean\"),\n",
    "        min_stock=(\"stock_cierre_num\",\"min\"),\n",
    "        max_stock=(\"stock_cierre_num\",\"max\"),\n",
    "        days_measured=(\"date_dt\",\"count\"),\n",
    "        days_stockout=(\"stock_cierre_num\", lambda s: (s.fillna(0) <= 0).sum()),\n",
    "        days_negative_stock=(\"stock_cierre_num\", lambda s: (s.fillna(0) < 0).sum()),\n",
    "    ).reset_index()\n",
    "\n",
    "    g[\"pct_days_stockout\"] = safe_ratio(g[\"days_stockout\"], g[\"days_measured\"])\n",
    "    return g\n",
    "\n",
    "stock_feat = compute_stock_features(stock) if stock is not None else None\n",
    "describe_quick(stock_feat, \"stock_feat (por producto)\", n=5)\n",
    "\n",
    "product_features_final = product_features.copy()\n",
    "if prod_dim is not None:\n",
    "    product_features_final = product_features_final.merge(prod_dim, on=\"product_id_std\", how=\"left\")\n",
    "if stock_feat is not None:\n",
    "    product_features_final = product_features_final.merge(stock_feat, on=\"product_id_std\", how=\"left\")\n",
    "\n",
    "describe_quick(product_features_final, \"product_features_final (enriquecido)\", n=10)\n",
    "\n",
    "# --- KPIs por categoría y canal (ventas)\n",
    "def compute_category_channel_kpis(fact_df: pd.DataFrame, prod_dim: Optional[pd.DataFrame] = None) -> pd.DataFrame:\n",
    "    df = fact_df[fact_df[\"is_sale\"]].copy()\n",
    "\n",
    "    if \"categoria\" not in df.columns and prod_dim is not None:\n",
    "        df = df.merge(prod_dim[[\"product_id_std\",\"categoria\",\"subcategoria\"]], on=\"product_id_std\", how=\"left\")\n",
    "\n",
    "    g = df.groupby([\"categoria\",\"canal_origen\"]).agg(\n",
    "        revenue=(\"amount_signed\",\"sum\"),\n",
    "        units=(\"quantity_signed\",\"sum\"),\n",
    "        docs=(\"doc_id_std\",\"nunique\"),\n",
    "        lines=(\"product_id_std\",\"size\"),\n",
    "        avg_discount=(\"discount_pct\",\"mean\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    return g\n",
    "\n",
    "category_channel = compute_category_channel_kpis(fact_t, prod_dim=prod_dim)\n",
    "describe_quick(category_channel, \"category_channel KPIs\", n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711a2e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================================\n",
    "# Preparación para análisis de cesta (Market Basket)\n",
    "# ==========================================================\n",
    "\n",
    "def build_basket_long(fact_df: pd.DataFrame,\n",
    "                      min_qty: float = 0.0,\n",
    "                      drop_duplicates_in_tx: bool = True) -> pd.DataFrame:\n",
    "    df = fact_df.copy()\n",
    "    df = df[df[\"is_sale\"]].copy()\n",
    "    df = df[df[\"product_id_std\"].notna() & df[\"doc_id_std\"].notna()].copy()\n",
    "    df = df[df[\"quantity_signed\"].fillna(0) > min_qty].copy()\n",
    "\n",
    "    basket = df[[\"doc_id_std\",\"product_id_std\",\"canal_origen\",\"event_dt\",\"quantity_signed\",\"amount_signed\"]].copy()\n",
    "    basket = basket.rename(columns={\"doc_id_std\":\"transaction_id\"})\n",
    "\n",
    "    if drop_duplicates_in_tx:\n",
    "        basket = basket.groupby([\"transaction_id\",\"product_id_std\",\"canal_origen\"], as_index=False).agg(\n",
    "            event_dt=(\"event_dt\",\"max\"),\n",
    "            quantity=(\"quantity_signed\",\"sum\"),\n",
    "            amount=(\"amount_signed\",\"sum\"),\n",
    "        )\n",
    "    else:\n",
    "        basket = basket.rename(columns={\"quantity_signed\":\"quantity\",\"amount_signed\":\"amount\"})\n",
    "\n",
    "    return basket\n",
    "\n",
    "basket_long = build_basket_long(fact_t)\n",
    "describe_quick(basket_long, \"basket_long\", n=10)\n",
    "\n",
    "print(\"Transacciones únicas:\", basket_long[\"transaction_id\"].nunique())\n",
    "print(\"Productos únicos:\", basket_long[\"product_id_std\"].nunique())\n",
    "\n",
    "# Ejemplo didáctico: matriz one-hot (puede crecer en casos reales)\n",
    "basket_onehot = (basket_long.assign(present=1)\n",
    "                          .pivot_table(index=\"transaction_id\", columns=\"product_id_std\", values=\"present\", aggfunc=\"max\", fill_value=0))\n",
    "print(\"Matriz one-hot:\", basket_onehot.shape)\n",
    "display(basket_onehot.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4523dee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================================\n",
    "# Dataset didáctico de propensión (por snapshots)\n",
    "# ==========================================================\n",
    "\n",
    "def compute_customer_features_in_window(fact_df: pd.DataFrame,\n",
    "                                        start: pd.Timestamp,\n",
    "                                        end: pd.Timestamp) -> pd.DataFrame:\n",
    "    \"\"\"Calcula features de cliente restringidas a un intervalo [start, end).\"\"\"\n",
    "    dfw = fact_df[(fact_df[\"event_dt\"] >= start) & (fact_df[\"event_dt\"] < end)].copy()\n",
    "    return compute_customer_features(dfw, as_of=end)\n",
    "\n",
    "def build_propensity_dataset(fact_df: pd.DataFrame,\n",
    "                             snapshot_dates: List[pd.Timestamp],\n",
    "                             lookback_days: int = 180,\n",
    "                             horizon_days: int = 60) -> pd.DataFrame:\n",
    "    fact_df = fact_df.copy()\n",
    "    fact_df = fact_df[fact_df[\"customer_id_std\"].notna()].copy()\n",
    "\n",
    "    rows = []\n",
    "    for snap in snapshot_dates:\n",
    "        snap = pd.to_datetime(snap)\n",
    "\n",
    "        start = snap - pd.Timedelta(days=lookback_days)\n",
    "        end = snap\n",
    "\n",
    "        feats = compute_customer_features_in_window(fact_df, start=start, end=end)\n",
    "\n",
    "        future_start = snap\n",
    "        future_end = snap + pd.Timedelta(days=horizon_days)\n",
    "\n",
    "        future_sales = (fact_df[(fact_df[\"is_sale\"]) &\n",
    "                                (fact_df[\"event_dt\"] >= future_start) &\n",
    "                                (fact_df[\"event_dt\"] < future_end)][[\"customer_id_std\",\"doc_id_std\"]]\n",
    "                        .drop_duplicates())\n",
    "        future_sales[\"label_buy_next_horizon\"] = 1\n",
    "\n",
    "        feats = feats.merge(future_sales[[\"customer_id_std\",\"label_buy_next_horizon\"]].drop_duplicates(),\n",
    "                            on=\"customer_id_std\", how=\"left\")\n",
    "        feats[\"label_buy_next_horizon\"] = feats[\"label_buy_next_horizon\"].fillna(0).astype(int)\n",
    "        feats[\"snapshot_date\"] = snap\n",
    "\n",
    "        rows.append(feats)\n",
    "\n",
    "    return pd.concat(rows, ignore_index=True) if rows else pd.DataFrame()\n",
    "\n",
    "max_dt = fact_t[\"event_dt\"].max()\n",
    "min_dt = fact_t[\"event_dt\"].min()\n",
    "\n",
    "if pd.notna(max_dt) and pd.notna(min_dt):\n",
    "    snapshots = pd.date_range(end=max_dt.normalize(), periods=6, freq=\"MS\")\n",
    "    snapshots = [d for d in snapshots if d > (min_dt + pd.Timedelta(days=240))]\n",
    "    print(\"Snapshots:\", snapshots)\n",
    "\n",
    "    propensity_ds = build_propensity_dataset(fact_t, snapshot_dates=snapshots, lookback_days=180, horizon_days=60)\n",
    "    describe_quick(propensity_ds, \"propensity_ds\", n=10)\n",
    "\n",
    "    print(\"Balance label_buy_next_horizon:\")\n",
    "    display(propensity_ds[\"label_buy_next_horizon\"].value_counts(normalize=True).rename(\"pct\"))\n",
    "else:\n",
    "    propensity_ds = pd.DataFrame()\n",
    "    print(\"No hay fechas suficientes para snapshots.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61296a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================================\n",
    "# Exportación de datasets de features (CSV)\n",
    "# ==========================================================\n",
    "\n",
    "def export_csv(df: pd.DataFrame, filename: str) -> str:\n",
    "    path = os.path.join(OUTPUT_DIR, filename)\n",
    "    df.to_csv(path, index=False, encoding=\"utf-8\")\n",
    "    print(\"Exportado:\", path, \"| filas:\", len(df))\n",
    "    return path\n",
    "\n",
    "export_csv(customer_features_final, \"features_clientes.csv\")\n",
    "export_csv(product_features_final, \"features_productos.csv\")\n",
    "export_csv(category_channel, \"features_categoria_canal.csv\")\n",
    "export_csv(basket_long, \"basket_long.csv\")\n",
    "\n",
    "if len(propensity_ds) > 0:\n",
    "    export_csv(propensity_ds, \"dataset_propension_snapshots.csv\")\n",
    "\n",
    "summary = pd.DataFrame([\n",
    "    {\"dataset\":\"features_clientes.csv\",\"rows\":len(customer_features_final)},\n",
    "    {\"dataset\":\"features_productos.csv\",\"rows\":len(product_features_final)},\n",
    "    {\"dataset\":\"features_categoria_canal.csv\",\"rows\":len(category_channel)},\n",
    "    {\"dataset\":\"basket_long.csv\",\"rows\":len(basket_long)},\n",
    "    {\"dataset\":\"dataset_propension_snapshots.csv\",\"rows\":len(propensity_ds)},\n",
    "])\n",
    "display(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5fce19",
   "metadata": {},
   "source": [
    "## Siguientes pasos sugeridos (para el curso)\n",
    "\n",
    "- **Churn / Abandono**\n",
    "  - Revisa la definición de etiqueta `label_churn_180d` (es didáctica).\n",
    "  - Prueba diferentes umbrales (90/120/180 días) o una definición basada en ventanas temporales.\n",
    "\n",
    "- **Propensión**\n",
    "  - Ajusta `lookback_days` y `horizon_days`.\n",
    "  - Puedes crear etiquetas por **categoría**: “comprará Muebles en los próximos 60 días”.\n",
    "\n",
    "- **Cesta (Market Basket)**\n",
    "  - Usa `basket_long.csv` para construir reglas de asociación (Apriori, FP-Growth).\n",
    "  - Prueba por canal (ONLINE vs POS) filtrando `canal_origen`.\n",
    "\n",
    "- **Notebook 6 – Preprocesamiento final (escala y codificación)**\n",
    "  - Escalado (MinMax / StandardScaler)\n",
    "  - One-hot encoding de `tier_fidelizacion`, `fav_channel`, etc.\n",
    "  - Dataset final numérico listo para ML/BI\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

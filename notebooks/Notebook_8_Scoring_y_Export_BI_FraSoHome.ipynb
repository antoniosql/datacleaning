{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "281d8d96",
   "metadata": {},
   "source": [
    "# Notebook 8 — *Scoring* y exportación para BI (FraSoHome)\n",
    "\n",
    "**Objetivo (formativo):** aplicar los modelos entrenados en el Notebook 7 sobre datasets *ML-ready* (salida del Notebook 6), generar ficheros de *scoring* para BI (PowerBI/Tableau) y crear explicaciones simples (drivers) para entender por qué un cliente/producto aparece con un score alto.\n",
    "\n",
    "> Nota: este notebook está diseñado para ser **robusto** con datos “sucios” (errores intencionales del caso) y con diferentes nombres de archivos. Si falta algún fichero, el notebook continúa y deja trazas claras de lo que no se pudo ejecutar.\n",
    "\n",
    "---\n",
    "\n",
    "## Entradas esperadas\n",
    "\n",
    "Carpetas típicas (si seguiste los notebooks 1–7):\n",
    "- `output_ml/`  \n",
    "  - `FraSoHome_clientes_ML_ready.csv`  \n",
    "  - `FraSoHome_propension_ML_ready.csv` *(si aplica)*  \n",
    "- `output_models/`  \n",
    "  - modelos `.joblib` guardados en el Notebook 7 (por ejemplo `best_churn_model.joblib`, `best_propension_model.joblib`)\n",
    "- `output_features/` *(opcional para enriquecer con IDs/atributos legibles)*  \n",
    "  - `features_clientes.csv`  \n",
    "  - `dataset_propension_snapshots.csv`\n",
    "\n",
    "## Salidas\n",
    "\n",
    "- `output_scoring/` con:\n",
    "  - `churn_scoring.csv` (si hay modelo de churn)\n",
    "  - `propension_scoring.csv` (si hay modelo de propensión)\n",
    "  - `*_global_drivers_top.csv` (top variables globales)\n",
    "  - `scoring_actions_dictionary.csv` (diccionario de acción por decil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b0ed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 0) Imports y configuración\n",
    "# =========================\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import glob\n",
    "import math\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Intentamos importar joblib. Si no está, usamos pickle como fallback.\n",
    "try:\n",
    "    import joblib\n",
    "    HAS_JOBLIB = True\n",
    "except Exception:\n",
    "    import pickle\n",
    "    HAS_JOBLIB = False\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "BASE_PATH = Path(\".\")  # Ajusta si ejecutas el notebook desde otro directorio\n",
    "OUTPUT_SCORING_DIR = BASE_PATH / \"output_scoring\"\n",
    "OUTPUT_SCORING_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUTPUT_MODELS_DIR = BASE_PATH / \"output_models\"\n",
    "OUTPUT_ML_DIR = BASE_PATH / \"output_ml\"\n",
    "OUTPUT_FEATURES_DIR = BASE_PATH / \"output_features\"\n",
    "\n",
    "SCORING_DATE = dt.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "print(\"HAS_JOBLIB:\", HAS_JOBLIB)\n",
    "print(\"BASE_PATH:\", BASE_PATH.resolve())\n",
    "print(\"OUTPUT_SCORING_DIR:\", OUTPUT_SCORING_DIR.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40824f5",
   "metadata": {},
   "source": [
    "## 1) Funciones reutilizables\n",
    "\n",
    "La idea es **encapsular la lógica** para que puedas:\n",
    "- reutilizar funciones con cualquier dataframe (parámetros),\n",
    "- testear piezas (lectura, alineación de columnas, scoring),\n",
    "- modificar reglas sin romper todo el notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55c10c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 1) Funciones reutilizables\n",
    "# =========================\n",
    "\n",
    "def safe_read_csv(path: Union[str, Path], dtype: str = \"str\", encoding: str = \"utf-8\") -> pd.DataFrame:\n",
    "    \"\"\"Lee un CSV en modo robusto (formativo).\n",
    "    - dtype=str para no “romper” con errores de tipos.\n",
    "    - keep_default_na=False para mantener valores como '' y strings raros si fuese necesario.\n",
    "    \"\"\"\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"No existe el fichero: {path}\")\n",
    "    return pd.read_csv(path, dtype=dtype, encoding=encoding, keep_default_na=False)\n",
    "\n",
    "def find_latest_file(patterns: List[str], folder: Union[str, Path]) -> Optional[Path]:\n",
    "    \"\"\"Busca el archivo más reciente (por mtime) dentro de una carpeta, dado un conjunto de patrones glob.\"\"\"\n",
    "    folder = Path(folder)\n",
    "    candidates: List[Path] = []\n",
    "    for pat in patterns:\n",
    "        candidates.extend([Path(p) for p in glob.glob(str(folder / pat))])\n",
    "    candidates = [p for p in candidates if p.exists()]\n",
    "    if not candidates:\n",
    "        return None\n",
    "    candidates.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return candidates[0]\n",
    "\n",
    "def load_model(path: Union[str, Path]):\n",
    "    \"\"\"Carga un modelo guardado. Soporta joblib o pickle.\"\"\"\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"No existe el modelo: {path}\")\n",
    "    if HAS_JOBLIB:\n",
    "        return joblib.load(path)\n",
    "    else:\n",
    "        with open(path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "def detect_target_column(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:\n",
    "    \"\"\"Devuelve la primera columna encontrada de una lista de candidatos.\"\"\"\n",
    "    cols = set(df.columns)\n",
    "    for c in candidates:\n",
    "        if c in cols:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def split_X_y(\n",
    "    df: pd.DataFrame,\n",
    "    target_candidates: List[str],\n",
    ") -> Tuple[pd.DataFrame, Optional[pd.Series], Optional[str]]:\n",
    "    \"\"\"Separa X e y si encuentra un target.\n",
    "    Devuelve (X, y_or_None, target_name_or_None)\n",
    "    \"\"\"\n",
    "    target = detect_target_column(df, target_candidates)\n",
    "    if target is None:\n",
    "        return df.copy(), None, None\n",
    "    y = df[target].copy()\n",
    "    X = df.drop(columns=[target]).copy()\n",
    "    return X, y, target\n",
    "\n",
    "def coerce_numeric_df(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convierte todas las columnas a numérico si es posible.\n",
    "    Asume que el dataset ML-ready debería ser totalmente numérico (excepto IDs).\n",
    "    \"\"\"\n",
    "    X2 = X.copy()\n",
    "    for c in X2.columns:\n",
    "        if X2[c].dtype == object:\n",
    "            # Intento suave: reemplazo de coma decimal por punto si parece número\n",
    "            s = X2[c].astype(str).str.strip()\n",
    "            s = s.str.replace(\"€\", \"\", regex=False).str.replace(\"EUR\", \"\", regex=False)\n",
    "            # caso coma decimal: 12,34 -> 12.34 (pero cuidado con miles, aquí asumimos ML-ready)\n",
    "            s = s.str.replace(\",\", \".\", regex=False)\n",
    "            X2[c] = pd.to_numeric(s, errors=\"ignore\")\n",
    "    # Si quedan objetos, intentamos forzar a NaN salvo IDs\n",
    "    return X2\n",
    "\n",
    "def identify_id_like_columns(df: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"Heurística para detectar columnas tipo ID/metadata que NO deberían entrar al modelo\n",
    "    (si entrenaste sin ellas). Aun así, si el modelo espera esas columnas, las alinearemos después.\n",
    "    \"\"\"\n",
    "    id_cols = []\n",
    "    for c in df.columns:\n",
    "        lc = c.lower()\n",
    "        if lc in {\"customer_id\", \"product_id\", \"order_id\", \"ticket_id\", \"store_id\", \"snapshot_date\"}:\n",
    "            id_cols.append(c); continue\n",
    "        if lc.endswith(\"_id\"):\n",
    "            id_cols.append(c); continue\n",
    "        if lc in {\"scoring_date\", \"model_name\"}:\n",
    "            id_cols.append(c); continue\n",
    "    return id_cols\n",
    "\n",
    "def align_columns_to_feature_names(X: pd.DataFrame, feature_names: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Alinea X a una lista de nombres de features (añade faltantes con 0, elimina extras).\n",
    "    Esto es CRÍTICO para scoring consistente.\n",
    "    \"\"\"\n",
    "    X_aligned = X.copy()\n",
    "    missing = [c for c in feature_names if c not in X_aligned.columns]\n",
    "    extra = [c for c in X_aligned.columns if c not in feature_names]\n",
    "    for c in missing:\n",
    "        X_aligned[c] = 0.0\n",
    "    if extra:\n",
    "        X_aligned = X_aligned.drop(columns=extra)\n",
    "    # reorden\n",
    "    X_aligned = X_aligned[feature_names]\n",
    "    return X_aligned\n",
    "\n",
    "def predict_proba_safe(model, X: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"Devuelve score de clase positiva.\n",
    "    - Si existe predict_proba: usa [:, 1]\n",
    "    - Si no existe, usa decision_function y lo pasa por sigmoide\n",
    "    - Si nada existe, usa predict y lo trata como {0,1}\n",
    "    \"\"\"\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        proba = model.predict_proba(X)\n",
    "        if proba.ndim == 2 and proba.shape[1] >= 2:\n",
    "            return proba[:, 1]\n",
    "        # caso raro: solo una columna\n",
    "        return proba.ravel()\n",
    "    if hasattr(model, \"decision_function\"):\n",
    "        z = model.decision_function(X)\n",
    "        # Sigmoide\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    # fallback\n",
    "    pred = model.predict(X)\n",
    "    return np.asarray(pred).astype(float)\n",
    "\n",
    "def compute_deciles(scores: pd.Series, n: int = 10) -> pd.Series:\n",
    "    \"\"\"Crea deciles (1..n). 1=menor score, n=mayor score.\"\"\"\n",
    "    s = pd.Series(scores).astype(float)\n",
    "    # qcut puede fallar si hay muchos valores iguales; duplicates='drop' reduce bins.\n",
    "    try:\n",
    "        return pd.qcut(s, q=n, labels=False, duplicates=\"drop\") + 1\n",
    "    except Exception:\n",
    "        # fallback: ranking\n",
    "        r = s.rank(method=\"average\", pct=True)\n",
    "        return (np.ceil(r * n)).clip(1, n).astype(int)\n",
    "\n",
    "def get_global_feature_importance(model, feature_names: List[str], top_n: int = 20) -> pd.DataFrame:\n",
    "    \"\"\"Extrae importancia global si el modelo la expone.\n",
    "    - Árboles: feature_importances_\n",
    "    - Lineales: abs(coef_)\n",
    "    \"\"\"\n",
    "    if hasattr(model, \"feature_importances_\"):\n",
    "        imp = np.asarray(model.feature_importances_).ravel()\n",
    "    elif hasattr(model, \"coef_\"):\n",
    "        coef = np.asarray(model.coef_).ravel()\n",
    "        imp = np.abs(coef)\n",
    "    else:\n",
    "        return pd.DataFrame(columns=[\"feature\", \"importance\"])\n",
    "    df_imp = pd.DataFrame({\"feature\": feature_names, \"importance\": imp})\n",
    "    df_imp = df_imp.sort_values(\"importance\", ascending=False).head(top_n).reset_index(drop=True)\n",
    "    return df_imp\n",
    "\n",
    "def explain_linear_row(model, x_row: pd.Series, feature_names: List[str], top_n: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"Explicación local sencilla para modelos lineales: contribución = coef * valor.\n",
    "    Útil para formación (no sustituye SHAP).\n",
    "    \"\"\"\n",
    "    if not hasattr(model, \"coef_\"):\n",
    "        return pd.DataFrame(columns=[\"feature\", \"value\", \"contribution\"])\n",
    "    coef = np.asarray(model.coef_).ravel()\n",
    "    x = x_row.reindex(feature_names).astype(float).values\n",
    "    contrib = coef * x\n",
    "    df = pd.DataFrame({\"feature\": feature_names, \"value\": x, \"contribution\": contrib})\n",
    "    df[\"abs_contribution\"] = df[\"contribution\"].abs()\n",
    "    df = df.sort_values(\"abs_contribution\", ascending=False).head(top_n).drop(columns=[\"abs_contribution\"])\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def save_csv(df: pd.DataFrame, path: Union[str, Path]) -> None:\n",
    "    path = Path(path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"✅ Guardado: {path}  | filas={len(df):,} cols={df.shape[1]}\")\n",
    "\n",
    "def print_head(df: pd.DataFrame, n: int = 5, title: str = \"\"):\n",
    "    if title:\n",
    "        print(\"\\n\" + \"=\"*len(title))\n",
    "        print(title)\n",
    "        print(\"=\"*len(title))\n",
    "    display(df.head(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11da8452",
   "metadata": {},
   "source": [
    "## 2) Carga de modelos y datasets ML-ready\n",
    "\n",
    "Convención (recomendada):\n",
    "- churn: `best_churn_model.joblib` + `FraSoHome_clientes_ML_ready.csv`\n",
    "- propensión: `best_propension_model.joblib` + `FraSoHome_propension_ML_ready.csv`\n",
    "\n",
    "Si los nombres difieren, buscamos por patrones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6903f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 2) Localizar y cargar modelos/datasets\n",
    "# =========================\n",
    "\n",
    "# Patrones típicos (ajusta si tus nombres son distintos)\n",
    "CHURN_MODEL_PATTERNS = [\n",
    "    \"*churn*model*.joblib\",\n",
    "    \"best_churn*.joblib\",\n",
    "    \"*churn*.joblib\",\n",
    "]\n",
    "\n",
    "PROP_MODEL_PATTERNS = [\n",
    "    \"*prop*model*.joblib\",\n",
    "    \"best_prop*.joblib\",\n",
    "    \"*propension*.joblib\",\n",
    "    \"*propensity*.joblib\",\n",
    "]\n",
    "\n",
    "churn_model_path = find_latest_file(CHURN_MODEL_PATTERNS, OUTPUT_MODELS_DIR)\n",
    "prop_model_path = find_latest_file(PROP_MODEL_PATTERNS, OUTPUT_MODELS_DIR)\n",
    "\n",
    "print(\"churn_model_path:\", churn_model_path)\n",
    "print(\"prop_model_path :\", prop_model_path)\n",
    "\n",
    "# Datasets ML-ready\n",
    "clientes_ml_path = (OUTPUT_ML_DIR / \"FraSoHome_clientes_ML_ready.csv\")\n",
    "prop_ml_path = (OUTPUT_ML_DIR / \"FraSoHome_propension_ML_ready.csv\")\n",
    "\n",
    "print(\"clientes_ml_path exists:\", clientes_ml_path.exists(), clientes_ml_path)\n",
    "print(\"prop_ml_path exists    :\", prop_ml_path.exists(), prop_ml_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec3643e",
   "metadata": {},
   "source": [
    "## 3) Scoring — Churn (clientes)\n",
    "\n",
    "### Salida para BI\n",
    "Generaremos un fichero con:\n",
    "- `customer_id` (si se puede recuperar)\n",
    "- `score_churn` (0–1)\n",
    "- `pred_churn` con un umbral configurable (por defecto 0.5)\n",
    "- `decil_churn` (1..10)\n",
    "- `top_drivers` (si el modelo es lineal, explicación local sencilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f972debb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 3) Scoring churn (clientes)\n",
    "# =========================\n",
    "\n",
    "CHURN_TARGET_CANDIDATES = [\"label_churn_180d\", \"churn\", \"target\", \"y\"]\n",
    "\n",
    "def score_dataset(\n",
    "    df_ml_ready: pd.DataFrame,\n",
    "    model_obj,\n",
    "    target_candidates: List[str],\n",
    "    id_cols_prefer: Optional[List[str]] = None,\n",
    "    threshold: float = 0.5,\n",
    "    model_name: str = \"model\",\n",
    "    top_n_drivers: int = 3,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Scoring genérico:\n",
    "    - soporta que el 'model_obj' sea:\n",
    "        a) estimator sklearn\n",
    "        b) dict {'model': estimator, 'feature_names': [...]}\n",
    "    Devuelve:\n",
    "      - scoring_df (BI-friendly)\n",
    "      - global_drivers_df (importancia global)\n",
    "    \"\"\"\n",
    "    # 1) Desempaquetar modelo si viene en dict\n",
    "    feature_names = None\n",
    "    model = model_obj\n",
    "    if isinstance(model_obj, dict):\n",
    "        model = model_obj.get(\"model\", model_obj)\n",
    "        feature_names = model_obj.get(\"feature_names\") or model_obj.get(\"columns\")  # alias\n",
    "        if feature_names is not None:\n",
    "            feature_names = list(feature_names)\n",
    "\n",
    "    # 2) Separar X/y si existe target\n",
    "    X_raw, y, y_name = split_X_y(df_ml_ready, target_candidates)\n",
    "\n",
    "    # 3) Identificadores (para BI)\n",
    "    id_cols = []\n",
    "    if id_cols_prefer:\n",
    "        id_cols = [c for c in id_cols_prefer if c in X_raw.columns]\n",
    "    # Si no se especifican, intentamos heurística\n",
    "    if not id_cols:\n",
    "        id_cols = identify_id_like_columns(X_raw)\n",
    "    id_df = X_raw[id_cols].copy() if id_cols else pd.DataFrame(index=X_raw.index)\n",
    "\n",
    "    # 4) Datos para el modelo\n",
    "    X = X_raw.drop(columns=id_cols, errors=\"ignore\").copy()\n",
    "    X = coerce_numeric_df(X)\n",
    "\n",
    "    # 5) Alinear columnas si tenemos feature_names\n",
    "    if feature_names is not None:\n",
    "        X = align_columns_to_feature_names(X, feature_names)\n",
    "\n",
    "    # 6) Convertir no-numéricos a NaN (y luego a 0 para scoring básico)\n",
    "    #    (en un pipeline real se usaría el mismo preprocesador del entrenamiento)\n",
    "    for c in X.columns:\n",
    "        if not np.issubdtype(X[c].dtype, np.number):\n",
    "            X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "    X = X.fillna(0.0)\n",
    "\n",
    "    # 7) Score\n",
    "    scores = predict_proba_safe(model, X)\n",
    "    scores = np.clip(scores, 0, 1)\n",
    "    scores_s = pd.Series(scores, index=X.index, name=\"score\")\n",
    "\n",
    "    # 8) Ensamblar salida BI\n",
    "    out = pd.DataFrame(index=X.index)\n",
    "    if not id_df.empty:\n",
    "        out = pd.concat([id_df.reset_index(drop=True), out.reset_index(drop=True)], axis=1)\n",
    "    out[\"score\"] = scores_s.reset_index(drop=True)\n",
    "    out[\"pred\"] = (out[\"score\"] >= threshold).astype(int)\n",
    "    out[\"decil\"] = compute_deciles(out[\"score\"], n=10)\n",
    "    out[\"model_name\"] = model_name\n",
    "    out[\"scoring_date\"] = SCORING_DATE\n",
    "    if y is not None:\n",
    "        # intentamos normalizar y a 0/1\n",
    "        y_num = pd.to_numeric(y, errors=\"coerce\").fillna(0).astype(int)\n",
    "        out[\"y_true\"] = y_num.reset_index(drop=True)\n",
    "\n",
    "    # 9) Drivers globales\n",
    "    feat_names_for_imp = feature_names if feature_names is not None else list(X.columns)\n",
    "    global_imp = get_global_feature_importance(model, feat_names_for_imp, top_n=25)\n",
    "    global_imp[\"model_name\"] = model_name\n",
    "\n",
    "    # 10) Drivers locales (solo para lineales)\n",
    "    if hasattr(model, \"coef_\") and top_n_drivers > 0:\n",
    "        # calcula top drivers para las N filas con score más alto\n",
    "        top_idx = out[\"score\"].sort_values(ascending=False).head(20).index\n",
    "        driver_cols = []\n",
    "        for i in top_idx:\n",
    "            expl = explain_linear_row(model, X.loc[i], feat_names_for_imp, top_n=top_n_drivers)\n",
    "            # convertimos a string para BI\n",
    "            driver_str = \"; \".join([f\"{r['feature']}({r['contribution']:+.3f})\" for _, r in expl.iterrows()])\n",
    "            driver_cols.append((i, driver_str))\n",
    "        driver_map = dict(driver_cols)\n",
    "        out[\"top_drivers_linear\"] = out.index.map(lambda i: driver_map.get(i, \"\"))\n",
    "\n",
    "    return out, global_imp\n",
    "\n",
    "# Ejecutar churn scoring si hay modelo y dataset\n",
    "churn_scoring_df = None\n",
    "churn_global_drivers = None\n",
    "\n",
    "if churn_model_path and clientes_ml_path.exists():\n",
    "    churn_model_obj = load_model(churn_model_path)\n",
    "    clientes_ml = safe_read_csv(clientes_ml_path, dtype=\"str\")\n",
    "\n",
    "    # Intento: recuperar customer_id si está en features_clientes (opcional)\n",
    "    # Si el ML-ready no lo contiene, no pasa nada: BI usará el índice.\n",
    "    # (Recomendación: en producción, conservar siempre una clave de negocio en el dataset de scoring)\n",
    "    churn_scoring_df, churn_global_drivers = score_dataset(\n",
    "        df_ml_ready=clientes_ml,\n",
    "        model_obj=churn_model_obj,\n",
    "        target_candidates=CHURN_TARGET_CANDIDATES,\n",
    "        id_cols_prefer=[\"customer_id\"],\n",
    "        threshold=0.50,\n",
    "        model_name=churn_model_path.stem,\n",
    "        top_n_drivers=3,\n",
    "    )\n",
    "\n",
    "    print(\"✅ churn_scoring_df generado:\", churn_scoring_df.shape)\n",
    "    print_head(churn_scoring_df.sort_values(\"score\", ascending=False), n=10, title=\"Top 10 churn scores (mayor riesgo)\")\n",
    "else:\n",
    "    print(\"⚠️ No se pudo ejecutar churn scoring (falta modelo o dataset ML-ready).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bf9569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export churn outputs\n",
    "if churn_scoring_df is not None:\n",
    "    save_csv(churn_scoring_df, OUTPUT_SCORING_DIR / \"churn_scoring.csv\")\n",
    "if churn_global_drivers is not None and not churn_global_drivers.empty:\n",
    "    save_csv(churn_global_drivers, OUTPUT_SCORING_DIR / \"churn_global_drivers_top.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47871583",
   "metadata": {},
   "source": [
    "## 4) Scoring — Propensión de compra\n",
    "\n",
    "Este bloque es análogo al churn, pero normalmente el dataset de propensión incluye:\n",
    "- `customer_id` (ideal)\n",
    "- `snapshot_date` (fecha de corte)\n",
    "- features agregadas de la ventana histórica\n",
    "- label futuro (en training) y **no** en scoring real\n",
    "\n",
    "Para scoring, si el fichero trae label, lo dejamos solo a efectos de evaluación didáctica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ac8133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 4) Scoring propensión\n",
    "# =========================\n",
    "\n",
    "PROP_TARGET_CANDIDATES = [\n",
    "    \"label_will_buy\", \"will_buy\", \"label_buy\", \"buy_next\", \"target\", \"y\"\n",
    "]\n",
    "\n",
    "prop_scoring_df = None\n",
    "prop_global_drivers = None\n",
    "\n",
    "if prop_model_path and prop_ml_path.exists():\n",
    "    prop_model_obj = load_model(prop_model_path)\n",
    "    prop_ml = safe_read_csv(prop_ml_path, dtype=\"str\")\n",
    "\n",
    "    prop_scoring_df, prop_global_drivers = score_dataset(\n",
    "        df_ml_ready=prop_ml,\n",
    "        model_obj=prop_model_obj,\n",
    "        target_candidates=PROP_TARGET_CANDIDATES,\n",
    "        id_cols_prefer=[\"customer_id\", \"snapshot_date\"],\n",
    "        threshold=0.50,\n",
    "        model_name=prop_model_path.stem,\n",
    "        top_n_drivers=3,\n",
    "    )\n",
    "\n",
    "    print(\"✅ prop_scoring_df generado:\", prop_scoring_df.shape)\n",
    "    print_head(prop_scoring_df.sort_values(\"score\", ascending=False), n=10, title=\"Top 10 propensión scores\")\n",
    "else:\n",
    "    print(\"⚠️ No se pudo ejecutar scoring de propensión (falta modelo o dataset ML-ready).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653b8a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export propensión outputs\n",
    "if prop_scoring_df is not None:\n",
    "    save_csv(prop_scoring_df, OUTPUT_SCORING_DIR / \"propension_scoring.csv\")\n",
    "if prop_global_drivers is not None and not prop_global_drivers.empty:\n",
    "    save_csv(prop_global_drivers, OUTPUT_SCORING_DIR / \"propension_global_drivers_top.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd18343",
   "metadata": {},
   "source": [
    "## 5) Diccionario de acciones para BI (ejemplo didáctico)\n",
    "\n",
    "Generamos una tabla simple para que BI pueda “traducir” deciles a acciones:\n",
    "- churn: decil 10 = riesgo máximo (acción proactiva)\n",
    "- propensión: decil 10 = alta probabilidad de compra (campañas de upsell/cross-sell)\n",
    "\n",
    "> Ajusta el texto al contexto del curso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5473bd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 5) Diccionario de acciones por decil\n",
    "# =========================\n",
    "\n",
    "def build_actions_dictionary(problem: str) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for decil in range(1, 11):\n",
    "        if problem == \"churn\":\n",
    "            if decil >= 9:\n",
    "                action = \"Contacto proactivo + oferta retención (alto riesgo)\"\n",
    "                priority = \"Alta\"\n",
    "            elif decil >= 6:\n",
    "                action = \"Re-engagement (email/app) + incentivo ligero (riesgo medio)\"\n",
    "                priority = \"Media\"\n",
    "            else:\n",
    "                action = \"Mantener comunicación estándar (riesgo bajo)\"\n",
    "                priority = \"Baja\"\n",
    "        elif problem == \"propension\":\n",
    "            if decil >= 9:\n",
    "                action = \"Campaña upsell/cross-sell (alta probabilidad compra)\"\n",
    "                priority = \"Alta\"\n",
    "            elif decil >= 6:\n",
    "                action = \"Recordatorio/remarketing suave (probabilidad media)\"\n",
    "                priority = \"Media\"\n",
    "            else:\n",
    "                action = \"No impactar / awareness (probabilidad baja)\"\n",
    "                priority = \"Baja\"\n",
    "        else:\n",
    "            action = \"N/A\"\n",
    "            priority = \"N/A\"\n",
    "        rows.append({\"problem\": problem, \"decil\": decil, \"priority\": priority, \"recommended_action\": action})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "actions_df = pd.concat([\n",
    "    build_actions_dictionary(\"churn\"),\n",
    "    build_actions_dictionary(\"propension\")\n",
    "], ignore_index=True)\n",
    "\n",
    "display(actions_df.head(12))\n",
    "save_csv(actions_df, OUTPUT_SCORING_DIR / \"scoring_actions_dictionary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6c2a0f",
   "metadata": {},
   "source": [
    "## 6) Recomendaciones prácticas (formativas)\n",
    "\n",
    "1. **Evitar leakage:** si tu dataset incluye columnas futuras (p.ej. `label_*` o métricas calculadas después del snapshot), deben excluirse al entrenar y al hacer scoring real.\n",
    "2. **Conservar claves de negocio:** para BI, es fundamental mantener `customer_id`, `product_id`, etc. En este notebook intentamos recuperarlas, pero lo ideal es que el pipeline las preserve.\n",
    "3. **Guardar metadatos del entrenamiento:** lista de columnas, umbral, fecha, versión del modelo, etc. Así se evita el “desalineado” de features.\n",
    "4. **Explicabilidad:** aquí usamos explicaciones sencillas (coeficientes / importancias). Para producción, puedes explorar técnicas como SHAP."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
